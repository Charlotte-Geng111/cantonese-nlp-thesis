{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gf60HoT633NY"
      },
      "source": [
        "# 使用 LLaMA Factory 微调 Llama-3 中文对话模型"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMp4rTWk4TKZ"
      },
      "source": [
        "## 安装 LLaMA Factory 依赖"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQDp0sXX3qE4",
        "outputId": "6833426b-6f9c-48f8-b7e7-82cf5027bf65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 353, done.\u001b[K\n",
            "remote: Counting objects: 100% (353/353), done.\u001b[K\n",
            "remote: Compressing objects: 100% (281/281), done.\u001b[K\n",
            "remote: Total 353 (delta 94), reused 184 (delta 57), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (353/353), 9.69 MiB | 29.90 MiB/s, done.\n",
            "Resolving deltas: 100% (94/94), done.\n",
            "/content/LLaMA-Factory\n",
            "\u001b[0m\u001b[01;34massets\u001b[0m/       \u001b[01;34mevaluation\u001b[0m/  MANIFEST.in     requirements.txt  \u001b[01;34mtests\u001b[0m/\n",
            "CITATION.cff  \u001b[01;34mexamples\u001b[0m/    pyproject.toml  \u001b[01;34mscripts\u001b[0m/\n",
            "\u001b[01;34mdata\u001b[0m/         LICENSE      README.md       setup.py\n",
            "\u001b[01;34mdocker\u001b[0m/       Makefile     README_zh.md    \u001b[01;34msrc\u001b[0m/\n",
            "Obtaining file:///content/LLaMA-Factory\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.51.2,>=4.41.2 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (4.50.3)\n",
            "Collecting datasets<=3.5.0,>=2.16.0 (from llamafactory==0.9.3.dev0)\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: accelerate<=1.6.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (1.5.2)\n",
            "Requirement already satisfied: peft<=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.14.0)\n",
            "Collecting trl<=0.9.6,>=0.8.6 (from llamafactory==0.9.3.dev0)\n",
            "  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting tokenizers<=0.21.0,>=0.19.0 (from llamafactory==0.9.3.dev0)\n",
            "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting gradio<=5.21.0,>=4.38.0 (from llamafactory==0.9.3.dev0)\n",
            "  Downloading gradio-5.21.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (1.14.1)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.8.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.2.0)\n",
            "Collecting tiktoken (from llamafactory==0.9.3.dev0)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (5.29.4)\n",
            "Collecting uvicorn (from llamafactory==0.9.3.dev0)\n",
            "  Downloading uvicorn-0.34.1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (2.11.2)\n",
            "Collecting fastapi (from llamafactory==0.9.3.dev0)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting sse-starlette (from llamafactory==0.9.3.dev0)\n",
            "  Downloading sse_starlette-2.2.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (3.10.0)\n",
            "Collecting fire (from llamafactory==0.9.3.dev0)\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (6.0.2)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (1.26.4)\n",
            "Collecting pydantic (from llamafactory==0.9.3.dev0)\n",
            "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting av (from llamafactory==0.9.3.dev0)\n",
            "  Downloading av-14.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (0.11.0)\n",
            "Collecting tyro<0.9.0 (from llamafactory==0.9.3.dev0)\n",
            "  Downloading tyro-0.8.14-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.11/dist-packages (from llamafactory==0.9.3.dev0) (2.6.0+cu124)\n",
            "Collecting bitsandbytes>=0.39.0 (from llamafactory==0.9.3.dev0)\n",
            "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (5.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (0.30.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.6.0,>=0.34.0->llamafactory==0.9.3.dev0) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (4.67.1)\n",
            "Collecting xxhash (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.11.15)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (4.9.0)\n",
            "Collecting ffmpy (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.7.2 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading gradio_client-1.7.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.1.6)\n",
            "Collecting markupsafe~=2.0 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.10.16)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (11.1.0)\n",
            "Collecting pydub (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting ruff>=0.9.3 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading ruff-0.11.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (2.10.0)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (4.13.1)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (15.0.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic->llamafactory==0.9.3.dev0) (0.7.0)\n",
            "Collecting pydantic-core==2.27.2 (from pydantic->llamafactory==0.9.3.dev0)\n",
            "  Downloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (3.4.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13.1->llamafactory==0.9.3.dev0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13.1->llamafactory==0.9.3.dev0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13.1->llamafactory==0.9.3.dev0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.1->llamafactory==0.9.3.dev0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.1->llamafactory==0.9.3.dev0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.1->llamafactory==0.9.3.dev0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.1->llamafactory==0.9.3.dev0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.1->llamafactory==0.9.3.dev0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.1->llamafactory==0.9.3.dev0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.1->llamafactory==0.9.3.dev0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->llamafactory==0.9.3.dev0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.1->llamafactory==0.9.3.dev0) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.51.2,>=4.41.2->llamafactory==0.9.3.dev0) (2024.11.6)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (13.9.4)\n",
            "Collecting shtab>=1.5.6 (from tyro<0.9.0->llamafactory==0.9.3.dev0)\n",
            "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn->llamafactory==0.9.3.dev0) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn->llamafactory==0.9.3.dev0) (0.14.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->llamafactory==0.9.3.dev0) (3.0.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.60.0)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.1.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (6.3.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (1.18.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.0.7)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa->llamafactory==0.9.3.dev0) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa->llamafactory==0.9.3.dev0) (4.3.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets<=3.5.0,>=2.16.0->llamafactory==0.9.3.dev0) (2.3.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (2.18.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa->llamafactory==0.9.3.dev0) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (1.17.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio<=5.21.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.5.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (0.1.2)\n",
            "Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-5.21.0-py3-none-any.whl (46.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.2/46.2 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.7.2-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.1/322.1 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.7/431.7 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m102.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.9.6-py3-none-any.whl (245 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.8.14-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading av-14.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.2/35.2 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sse_starlette-2.2.1-py3-none-any.whl (10 kB)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llamafactory, fire\n",
            "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llamafactory: filename=llamafactory-0.9.3.dev0-0.editable-py3-none-any.whl size=26401 sha256=aab4ef97da11d8875668e2d5604e9330e4fa307e416d9214ef99563b043e622b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-54wdwdlw/wheels/bd/34/05/1e3cb4b8f20c20631b411dc5157b4b150850c03496fa96c2c4\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=e135630f3fa80edcf5302a2d87d1ad7223157031199690c2146bc399b4e756d7\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
            "Successfully built llamafactory fire\n",
            "Installing collected packages: pydub, xxhash, uvicorn, tomlkit, shtab, ruff, python-multipart, pydantic-core, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, markupsafe, groovy, fsspec, fire, ffmpy, dill, av, aiofiles, tiktoken, starlette, pydantic, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, tyro, tokenizers, sse-starlette, safehttpx, nvidia-cusolver-cu12, gradio-client, fastapi, gradio, datasets, bitsandbytes, trl, llamafactory\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.33.1\n",
            "    Uninstalling pydantic_core-2.33.1:\n",
            "      Successfully uninstalled pydantic_core-2.33.1\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.11.2\n",
            "    Uninstalling pydantic-2.11.2:\n",
            "      Successfully uninstalled pydantic-2.11.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.1\n",
            "    Uninstalling tokenizers-0.21.1:\n",
            "      Successfully uninstalled tokenizers-0.21.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiofiles-23.2.1 av-14.3.0 bitsandbytes-0.45.5 datasets-3.5.0 dill-0.3.8 fastapi-0.115.12 ffmpy-0.5.0 fire-0.7.0 fsspec-2024.12.0 gradio-5.21.0 gradio-client-1.7.2 groovy-0.1.2 llamafactory-0.9.3.dev0 markupsafe-2.1.5 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pydantic-2.10.6 pydantic-core-2.27.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.5 safehttpx-0.1.6 shtab-1.7.2 sse-starlette-2.2.1 starlette-0.46.2 tiktoken-0.9.0 tokenizers-0.21.0 tomlkit-0.13.2 trl-0.9.6 tyro-0.8.14 uvicorn-0.34.1 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "%cd /content/\n",
        "%rm -rf LLaMA-Factory\n",
        "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
        "%cd LLaMA-Factory\n",
        "%ls\n",
        "!pip install -e .[torch,bitsandbytes]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gs68aSwm5MFi"
      },
      "source": [
        "### 检查 GPU 环境"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6rwbyFa5LkF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "try:\n",
        "  assert torch.cuda.is_available() is True\n",
        "except AssertionError:\n",
        "  print(\"需要 GPU 环境\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "AOksCVlOGJba",
        "outputId": "7e21ef12-2969-4c7e-f084-d82e3fcfe436"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting predibase\n",
            "  Downloading predibase-2025.3.2.tar.gz (93 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/93.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting dataclasses-json==0.5.7 (from predibase)\n",
            "  Downloading dataclasses_json-0.5.7-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting deprecation (from predibase)\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting ipyplot (from predibase)\n",
            "  Downloading ipyplot-1.1.2-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: ipython!=8.13.0 in /usr/local/lib/python3.11/dist-packages (from predibase) (7.34.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from predibase) (2.2.2)\n",
            "Collecting predibase-api==2025.3.2 (from predibase)\n",
            "  Downloading predibase-api-2025.3.2.tar.gz (6.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting progress-table==0.1.26 (from predibase)\n",
            "  Downloading progress-table-0.1.26.tar.gz (12 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from predibase) (5.29.4)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (from predibase) (18.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from predibase) (2.11.2)\n",
            "Requirement already satisfied: pyjwt in /usr/local/lib/python3.11/dist-packages (from predibase) (2.10.1)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from predibase) (2.8.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from predibase) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from predibase) (2.32.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from predibase) (13.9.4)\n",
            "Collecting semantic-version (from predibase)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from predibase) (0.9.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (from predibase) (2.18.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from predibase) (4.67.1)\n",
            "Collecting tritonclient (from predibase)\n",
            "  Downloading tritonclient-2.56.0-py3-none-manylinux1_x86_64.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: typer[all] in /usr/local/lib/python3.11/dist-packages (from predibase) (0.15.2)\n",
            "Requirement already satisfied: urllib3>=1.26.12 in /usr/local/lib/python3.11/dist-packages (from predibase) (2.3.0)\n",
            "Requirement already satisfied: websockets>=11.0.3 in /usr/local/lib/python3.11/dist-packages (from predibase) (15.0.1)\n",
            "Collecting lorax-client>=0.6.1 (from predibase)\n",
            "  Downloading lorax_client-0.6.3-py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting eval-type-backport (from predibase)\n",
            "  Downloading eval_type_backport-0.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (from predibase) (1.70.0)\n",
            "Collecting retry (from predibase)\n",
            "  Downloading retry-0.9.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.3.0 (from dataclasses-json==0.5.7->predibase)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting marshmallow-enum<2.0.0,>=1.5.1 (from dataclasses-json==0.5.7->predibase)\n",
            "  Downloading marshmallow_enum-1.5.1-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting typing-inspect>=0.4.0 (from dataclasses-json==0.5.7->predibase)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting colorama (from progress-table==0.1.26->predibase)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython!=8.13.0->predibase) (75.2.0)\n",
            "Collecting jedi>=0.16 (from ipython!=8.13.0->predibase)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython!=8.13.0->predibase) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython!=8.13.0->predibase) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython!=8.13.0->predibase) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython!=8.13.0->predibase) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython!=8.13.0->predibase) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython!=8.13.0->predibase) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython!=8.13.0->predibase) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython!=8.13.0->predibase) (4.9.0)\n",
            "Requirement already satisfied: aiohttp<4.0,>=3.9 in /usr/local/lib/python3.11/dist-packages (from lorax-client>=0.6.1->predibase) (3.11.15)\n",
            "Collecting certifi==2024.7.4 (from lorax-client>=0.6.1->predibase)\n",
            "  Downloading certifi-2024.7.4-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from lorax-client>=0.6.1->predibase) (0.30.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->predibase) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->predibase) (2.33.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->predibase) (4.13.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->predibase) (0.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from deprecation->predibase) (24.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ipyplot->predibase) (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from ipyplot->predibase) (11.1.0)\n",
            "Collecting shortuuid (from ipyplot->predibase)\n",
            "  Downloading shortuuid-1.0.13-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai->predibase) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai->predibase) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai->predibase) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai->predibase) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai->predibase) (1.3.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->predibase) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->predibase) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->predibase) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->predibase) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->predibase) (3.10)\n",
            "Collecting py<2.0.0,>=1.4.26 (from retry->predibase)\n",
            "  Downloading py-1.11.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->predibase) (3.0.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard->predibase) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard->predibase) (1.71.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard->predibase) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->predibase) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard->predibase) (3.1.3)\n",
            "Collecting numpy (from ipyplot->predibase)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-rapidjson>=0.9.1 (from tritonclient->predibase)\n",
            "  Downloading python_rapidjson-1.20-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "\u001b[33mWARNING: typer 0.15.2 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer[all]->predibase) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer[all]->predibase) (1.5.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0,>=3.9->lorax-client>=0.6.1->predibase) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0,>=3.9->lorax-client>=0.6.1->predibase) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0,>=3.9->lorax-client>=0.6.1->predibase) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0,>=3.9->lorax-client>=0.6.1->predibase) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0,>=3.9->lorax-client>=0.6.1->predibase) (6.3.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0,>=3.9->lorax-client>=0.6.1->predibase) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0,>=3.9->lorax-client>=0.6.1->predibase) (1.18.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai->predibase) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->predibase) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.12->lorax-client>=0.6.1->predibase) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.12->lorax-client>=0.6.1->predibase) (2025.3.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython!=8.13.0->predibase) (0.8.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->predibase) (0.1.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython!=8.13.0->predibase) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython!=8.13.0->predibase) (0.2.13)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.4.0->dataclasses-json==0.5.7->predibase)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard->predibase) (3.0.2)\n",
            "Downloading dataclasses_json-0.5.7-py3-none-any.whl (25 kB)\n",
            "Downloading lorax_client-0.6.3-py3-none-any.whl (12 kB)\n",
            "Downloading certifi-2024.7.4-py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.0/163.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading eval_type_backport-0.2.2-py3-none-any.whl (5.8 kB)\n",
            "Downloading ipyplot-1.1.2-py3-none-any.whl (13 kB)\n",
            "Downloading retry-0.9.2-py2.py3-none-any.whl (8.0 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading tritonclient-2.56.0-py3-none-manylinux1_x86_64.whl (14.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.4/14.4 MB\u001b[0m \u001b[31m106.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow_enum-1.5.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_rapidjson-1.20-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading shortuuid-1.0.13-py3-none-any.whl (10 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: predibase, predibase-api, progress-table\n",
            "  Building wheel for predibase (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for predibase: filename=predibase-2025.3.2-py3-none-any.whl size=121356 sha256=aeeea14f6ea10b76afa552c8525deb1bef26434d42c413a25169c39646ebdd2b\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/70/10/76de7ec289339da027edac3a161becd538be680cd0f1145554\n",
            "  Building wheel for predibase-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for predibase-api: filename=predibase_api-2025.3.2-py3-none-any.whl size=10266 sha256=65ed0a9953a0ed0f74a1843112532cf23d476b86c2c045830a332f3e31149791\n",
            "  Stored in directory: /root/.cache/pip/wheels/cb/a6/2f/602b948d10a811720b583b9b8e1d3e54a5491703407f41e0bf\n",
            "  Building wheel for progress-table (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for progress-table: filename=progress_table-0.1.26-py3-none-any.whl size=12311 sha256=8ea2207f560a7c160607d40cf611009a37272bafecff75a62b45a2266c94b1b1\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/74/8d/a81415685be0a00425619b2ca39a794b42b1d2a86cd20e7c1f\n",
            "Successfully built predibase predibase-api progress-table\n",
            "Installing collected packages: shortuuid, semantic-version, python-rapidjson, py, predibase-api, numpy, mypy-extensions, marshmallow, jedi, eval-type-backport, deprecation, colorama, certifi, typing-inspect, tritonclient, retry, progress-table, marshmallow-enum, ipyplot, dataclasses-json, lorax-client, predibase\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2025.1.31\n",
            "    Uninstalling certifi-2025.1.31:\n",
            "      Successfully uninstalled certifi-2025.1.31\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed certifi-2024.7.4 colorama-0.4.6 dataclasses-json-0.5.7 deprecation-2.1.0 eval-type-backport-0.2.2 ipyplot-1.1.2 jedi-0.19.2 lorax-client-0.6.3 marshmallow-3.26.1 marshmallow-enum-1.5.1 mypy-extensions-1.0.0 numpy-1.26.4 predibase-2025.3.2 predibase-api-2025.3.2 progress-table-0.1.26 py-1.11.0 python-rapidjson-1.20 retry-0.9.2 semantic-version-2.10.0 shortuuid-1.0.13 tritonclient-2.56.0 typing-inspect-0.9.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi"
                ]
              },
              "id": "b17949e0b55b41ecba0edb2237b7274f"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install predibase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yYTAxvIGHgE",
        "outputId": "c511734c-b93c-4ab7-f771-7922e0ea4c95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predibase 已安装并可用！\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import predibase as pb\n",
        "    print(\"Predibase 已安装并可用！\")\n",
        "except ImportError:\n",
        "    print(\"Predibase 未安装，请先安装。\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzYsragfYLbN",
        "outputId": "de048280-b060-4c18-af2e-62177ece324b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge_chinese\n",
            "  Downloading rouge_chinese-1.0.3-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from rouge_chinese) (1.17.0)\n",
            "Downloading rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: rouge_chinese\n",
            "Successfully installed rouge_chinese-1.0.3\n"
          ]
        }
      ],
      "source": [
        "pip install rouge_chinese"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYvbuTsUYoP5",
        "outputId": "601abf29-28e3-4c86-b8ac-69792ac447e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rouge_chinese installed successfully!\n"
          ]
        }
      ],
      "source": [
        "!python -c \"import rouge_chinese; print('rouge_chinese installed successfully!')\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYEF3SGI6XdO"
      },
      "source": [
        "## 更新自我认知数据集\n",
        "\n",
        "可以自由修改 NAME 和 AUTHOR 变量的内容。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gX4PskL6UJP",
        "outputId": "b77a7c15-28d5-455f-e73d-ec7a8b2e742a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "%cd /content/LLaMA-Factory/\n",
        "\n",
        "NAME = \"Llama-Chinese\"\n",
        "AUTHOR = \"LLaMA Factory\"\n",
        "\n",
        "with open(\"data/identity.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "  dataset = json.load(f)\n",
        "\n",
        "for sample in dataset:\n",
        "  sample[\"output\"] = sample[\"output\"].replace(\"{{\"+ \"name\" + \"}}\", NAME).replace(\"{{\"+ \"author\" + \"}}\", AUTHOR)\n",
        "\n",
        "with open(\"data/identity.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "  json.dump(dataset, f, indent=2, ensure_ascii=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cA7vWZ6om3cR"
      },
      "source": [
        "## 使用 LLaMA Board Web UI 微调模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "4e69c10ea9b34a54b7bbffb1fe360a7a",
            "2fe96ee737c64d9b8011018d04205d19",
            "09708cceb7244359961cc36ef0dc0ccb",
            "fb776a80517d4fa19ecc5ecb8d49ed8f",
            "b0b8270eddb24107b2c511fa105b8dc6",
            "aae756c717ea41b09f16da68fd2d5481",
            "1ef7bcac08ba444ebf2d980391a36e0c",
            "3f12f7ba7f49416abc1b73005ee1d616",
            "8d96e2391bc94f94b83707048de46730",
            "06a9d42bc1f84c1488f3fe9f7191588c",
            "e09c15b7dd2a4a7793b15680e0621b18",
            "2be2ffc36d794580a02b702c34d93471",
            "a60023e5979645b3ae90e8eeaa1ef595",
            "91c9269f80fc400a8449057b07345c16",
            "5cd71d1da66f45e3adddfc6811929b31",
            "5dbb89ed0a114202be3742f4bf0efdb5",
            "38b4972dc534429e9e3a4aa6a244b715",
            "e55899ce73c14a3ab3052c2b406a8f88",
            "7ebdd055fdfc447faef1c1ff26c20125",
            "0d742f3d3a9f4423a77b465338550ca1"
          ]
        },
        "id": "eUvXlCSXGNgS",
        "outputId": "cb62daeb-19bf-4bed-999a-91dbbfb20d90"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4e69c10ea9b34a54b7bbffb1fe360a7a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIfzFgLsm2kS",
        "outputId": "cafc7152-b51f-4f0a-cd9c-51cc74f793c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n",
            "2025-04-14 07:08:57.376685: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-14 07:08:57.395304: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744614537.417230    3533 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744614537.424013    3533 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-14 07:08:57.446363: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Visit http://ip:port for Web UI, e.g., http://127.0.0.1:7860\n",
            "* Running on local URL:  http://0.0.0.0:7860\n",
            "* Running on public URL: https://37a019eefe6495ca24.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "2025-04-14 07:11:10.646892: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744614670.668258    4141 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744614670.674909    4141 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "[INFO|2025-04-14 07:11:17] llamafactory.hparams.parser:379 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.bfloat16\n",
            "tokenizer_config.json: 100% 1.53k/1.53k [00:00<00:00, 12.2MB/s]\n",
            "vocab.json: 100% 2.78M/2.78M [00:00<00:00, 25.9MB/s]\n",
            "merges.txt: 100% 1.67M/1.67M [00:00<00:00, 23.9MB/s]\n",
            "tokenizer.json: 100% 7.03M/7.03M [00:00<00:00, 24.1MB/s]\n",
            "added_tokens.json: 100% 80.0/80.0 [00:00<00:00, 764kB/s]\n",
            "special_tokens_map.json: 100% 367/367 [00:00<00:00, 3.55MB/s]\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 07:11:18,416 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 07:11:18,417 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 07:11:18,417 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 07:11:18,417 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 07:11:18,417 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 07:11:18,417 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 07:11:18,417 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2323] 2025-04-14 07:11:18,792 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "config.json: 100% 771/771 [00:00<00:00, 8.08MB/s]\n",
            "[INFO|configuration_utils.py:699] 2025-04-14 07:11:19,041 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-04-14 07:11:19,043 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3584,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 18944,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 28,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 4,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 131072,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.50.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 152064\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 07:11:19,086 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 07:11:19,086 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 07:11:19,086 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 07:11:19,086 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 07:11:19,086 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 07:11:19,086 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 07:11:19,086 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2323] 2025-04-14 07:11:19,432 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|2025-04-14 07:11:19] llamafactory.data.loader:143 >> Loading dataset Openrice_train_alpaca_with_emojis.json...\n",
            "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
            "Generating train split: 11884 examples [00:00, 63346.78 examples/s]\n",
            "Converting format of dataset (num_proc=16): 100% 11884/11884 [00:00<00:00, 31017.05 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16): 100% 11884/11884 [00:03<00:00, 3001.93 examples/s]\n",
            "training example:\n",
            "input_ids:\n",
            "[33975, 25, 220, 56568, 101909, 101897, 104034, 100623, 48692, 100168, 3837, 73670, 101042, 108704, 31196, 90395, 100345, 102285, 16744, 105278, 70538, 17714, 117585, 109955, 31905, 1773, 103929, 88802, 20412, 104857, 102086, 31196, 62926, 60610, 31196, 100409, 113195, 104405, 1773, 45181, 87752, 104405, 15946, 50404, 5122, 16, 13, 1036, 106557, 33590, 17, 34047, 103276, 55807, 91680, 99553, 104405, 9370, 29991, 100622, 66017, 3837, 16530, 99553, 108593, 9370, 104136, 8997, 104064, 61443, 99450, 63379, 112512, 113049, 100039, 99627, 109379, 100220, 107380, 23, 99699, 106372, 20, 19, 15, 107380, 99512, 69249, 69103, 44636, 111199, 68536, 106898, 112109, 109692, 99165, 104838, 101885, 101229, 101677, 99465, 102775, 117874, 102634, 104334, 111391, 100038, 99665, 17447, 114443, 111770, 106795, 16530, 102634, 29767, 102003, 101940, 99555, 101229, 103931, 108471, 103347, 101364, 99796, 107510, 101097, 99213, 77540, 14224, 80443, 104108, 81800, 80443, 115688, 101229, 109692, 99405, 104197, 101929, 100131, 106099, 91680, 99992, 100749, 100399, 46306, 80158, 101043, 52510, 34187, 100626, 112292, 104000, 99470, 47534, 102125, 100081, 100475, 27442, 9370, 103963, 99491, 105952, 100380, 16530, 105874, 112822, 104886, 99665, 44729, 103972, 99165, 100667, 105140, 100141, 99278, 20929, 14777, 9370, 71416, 101103, 100137, 47874, 109984, 55286, 80443, 101958, 100371, 45181, 104722, 100766, 100688, 104150, 107661, 45181, 100106, 99958, 100896, 113715, 30440, 101041, 109239, 151645, 198, 71703, 25, 106557, 151645, 198]\n",
            "inputs:\n",
            "Human: 你是一个经过训练的人工智能，可以分析文本输入，并根据上下文将其分类为最合适的情感类型。你的任务是仔细评估输入并确定输入属于哪种情绪。从以下情绪中选择：1. “正面”，2.“负面”。只提供情绪的名称作为输出，不提供额外的解释。\n",
            "第一次写食评赶上狂欢夏威夷主题自助8折人均540自助餐里算高价位而性价比非常高海鲜很新鲜而且供应不停龙虾钳剪好了摆在冰盘上也是很良心明明不剪开就可以减少很多供应除了煎鹅肝一次排队只能领两件没有限制数量没有削减供应海鲜吃的大满足但是饮料只有一杯喝完就只有水了还有草莓朱古力喷泉甜点的制作非常用心一点不随意服务员清理盘子亦很及时到位一般带加一的店都有这种服务就好了开始没有找到地方从地面绕过去回来才知道从朗豪坊楼上可直接穿过<|im_end|>\n",
            "Assistant:正面<|im_end|>\n",
            "\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 106557, 151645, 198]\n",
            "labels:\n",
            "正面<|im_end|>\n",
            "\n",
            "[INFO|configuration_utils.py:699] 2025-04-14 07:11:24,978 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-04-14 07:11:24,979 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3584,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 18944,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 28,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 4,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 131072,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.50.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 152064\n",
            "}\n",
            "\n",
            "[INFO|2025-04-14 07:11:24] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n",
            "model.safetensors.index.json: 100% 27.8k/27.8k [00:00<00:00, 108MB/s]\n",
            "[INFO|modeling_utils.py:1154] 2025-04-14 07:11:25,481 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/model.safetensors.index.json\n",
            "Fetching 4 files:   0% 0/4 [00:00<?, ?it/s]\n",
            "model-00004-of-00004.safetensors:   0% 0.00/3.56G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:   0% 0.00/3.95G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   0% 0.00/3.86G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:   0% 0.00/3.86G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:   0% 10.5M/3.95G [00:00<01:23, 47.3MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:   0% 10.5M/3.56G [00:00<01:18, 45.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   0% 10.5M/3.86G [00:00<01:24, 45.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:   0% 10.5M/3.86G [00:00<01:23, 46.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   1% 21.0M/3.86G [00:00<01:02, 61.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:   1% 31.5M/3.56G [00:00<00:38, 92.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:   1% 31.5M/3.95G [00:00<00:43, 90.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:   1% 21.0M/3.86G [00:00<01:07, 56.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   1% 31.5M/3.86G [00:00<00:53, 72.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:   1% 52.4M/3.56G [00:00<00:30, 116MB/s] \u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:   1% 52.4M/3.95G [00:00<00:34, 113MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:   1% 31.5M/3.86G [00:00<00:56, 68.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   1% 52.4M/3.86G [00:00<00:38, 97.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:   2% 73.4M/3.56G [00:00<00:26, 130MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:   2% 73.4M/3.95G [00:00<00:30, 128MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:   1% 52.4M/3.86G [00:00<00:40, 94.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   2% 73.4M/3.86G [00:00<00:31, 122MB/s] \u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:   3% 94.4M/3.56G [00:00<00:24, 139MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:   2% 94.4M/3.95G [00:00<00:28, 136MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:   2% 62.9M/3.86G [00:00<00:40, 94.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   2% 94.4M/3.86G [00:00<00:27, 137MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:   3% 115M/3.56G [00:00<00:23, 144MB/s] \u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:   3% 115M/3.95G [00:00<00:26, 142MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:   2% 73.4M/3.86G [00:00<00:42, 89.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:   4% 136M/3.56G [00:01<00:22, 149MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:   3% 136M/3.95G [00:01<00:26, 146MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   3% 115M/3.86G [00:01<00:29, 128MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:   2% 83.9M/3.86G [00:01<00:42, 88.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:   4% 157M/3.56G [00:01<00:22, 149MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:   4% 157M/3.95G [00:01<00:25, 148MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:   2% 94.4M/3.86G [00:01<00:43, 86.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   4% 136M/3.86G [00:01<00:30, 123MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:   5% 178M/3.56G [00:01<00:22, 149MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:   5% 178M/3.95G [00:01<00:25, 150MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:   3% 105M/3.86G [00:01<00:43, 86.1MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   4% 157M/3.86G [00:01<00:31, 119MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:   6% 199M/3.56G [00:01<00:22, 150MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:   5% 199M/3.95G [00:01<00:24, 150MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:   3% 115M/3.86G [00:01<00:44, 84.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:   6% 220M/3.56G [00:01<00:21, 152MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:   6% 220M/3.95G [00:01<00:24, 151MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:   3% 126M/3.86G [00:01<00:44, 84.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   5% 178M/3.86G [00:01<00:31, 117MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:   7% 241M/3.56G [00:01<00:21, 155MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:   4% 136M/3.86G [00:01<00:44, 84.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:   6% 241M/3.95G [00:01<00:24, 151MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   5% 199M/3.86G [00:01<00:31, 117MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:   7% 262M/3.56G [00:01<00:21, 154MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:   4% 147M/3.86G [00:01<00:44, 83.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:   7% 262M/3.95G [00:01<00:24, 152MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   6% 220M/3.86G [00:01<00:31, 115MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:   8% 283M/3.56G [00:02<00:21, 154MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:   4% 157M/3.86G [00:01<00:44, 83.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:   7% 283M/3.95G [00:02<00:24, 152MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:   4% 168M/3.86G [00:02<00:44, 83.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:   9% 304M/3.56G [00:02<00:20, 155MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:   8% 304M/3.95G [00:02<00:24, 151MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   6% 241M/3.86G [00:02<00:31, 115MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:   5% 178M/3.86G [00:02<00:44, 82.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:   9% 325M/3.56G [00:02<00:21, 154MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:   8% 325M/3.95G [00:02<00:24, 150MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   7% 262M/3.86G [00:02<00:31, 114MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:   5% 189M/3.86G [00:02<00:44, 83.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  10% 346M/3.56G [00:02<00:20, 154MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:   9% 346M/3.95G [00:02<00:24, 150MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:   5% 199M/3.86G [00:02<00:44, 82.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   7% 283M/3.86G [00:02<00:31, 114MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  10% 367M/3.56G [00:02<00:20, 153MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:   9% 367M/3.95G [00:02<00:23, 150MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:   5% 210M/3.86G [00:02<00:44, 82.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  11% 388M/3.56G [00:02<00:21, 151MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  10% 388M/3.95G [00:02<00:23, 150MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   8% 304M/3.86G [00:02<00:31, 112MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:   6% 220M/3.86G [00:02<00:43, 83.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  10% 409M/3.95G [00:02<00:23, 148MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  11% 409M/3.56G [00:02<00:23, 134MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:   6% 231M/3.86G [00:02<00:44, 82.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   8% 325M/3.86G [00:02<00:33, 106MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  11% 430M/3.95G [00:03<00:23, 149MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:   6% 241M/3.86G [00:02<00:43, 82.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  12% 430M/3.56G [00:03<00:22, 140MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:   7% 252M/3.86G [00:03<00:43, 83.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   9% 346M/3.86G [00:03<00:32, 108MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  11% 451M/3.95G [00:03<00:23, 149MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  13% 451M/3.56G [00:03<00:21, 144MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  12% 472M/3.95G [00:03<00:23, 151MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  13% 472M/3.56G [00:03<00:20, 148MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:   7% 262M/3.86G [00:03<00:46, 76.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:   9% 367M/3.86G [00:03<00:31, 110MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  12% 493M/3.95G [00:03<00:22, 152MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  14% 493M/3.56G [00:03<00:20, 150MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:   7% 273M/3.86G [00:03<00:46, 77.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  10% 388M/3.86G [00:03<00:31, 111MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  13% 514M/3.95G [00:03<00:22, 152MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  14% 514M/3.56G [00:03<00:20, 151MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:   7% 283M/3.86G [00:03<00:45, 77.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:   8% 294M/3.86G [00:03<00:44, 79.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  11% 409M/3.86G [00:03<00:30, 112MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  14% 535M/3.95G [00:03<00:22, 151MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  15% 535M/3.56G [00:03<00:19, 151MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:   8% 304M/3.86G [00:03<00:44, 80.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  14% 556M/3.95G [00:03<00:22, 152MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  16% 556M/3.56G [00:03<00:19, 152MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  11% 430M/3.86G [00:03<00:30, 113MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:   8% 315M/3.86G [00:03<00:43, 81.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  15% 577M/3.95G [00:03<00:22, 151MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  16% 577M/3.56G [00:03<00:19, 151MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  12% 451M/3.86G [00:04<00:30, 112MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:   8% 325M/3.86G [00:03<00:43, 81.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  17% 598M/3.56G [00:04<00:19, 153MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  15% 598M/3.95G [00:04<00:22, 149MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:   9% 336M/3.86G [00:04<00:43, 81.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  17% 619M/3.56G [00:04<00:19, 154MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  12% 472M/3.86G [00:04<00:30, 113MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  16% 619M/3.95G [00:04<00:22, 149MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:   9% 346M/3.86G [00:04<00:42, 82.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  18% 640M/3.56G [00:04<00:18, 154MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  16% 640M/3.95G [00:04<00:22, 149MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  13% 493M/3.86G [00:04<00:29, 113MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:   9% 357M/3.86G [00:04<00:42, 82.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  19% 661M/3.56G [00:04<00:18, 155MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  17% 661M/3.95G [00:04<00:22, 149MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:   9% 367M/3.86G [00:04<00:42, 82.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  13% 514M/3.86G [00:04<00:30, 111MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  19% 682M/3.56G [00:04<00:18, 154MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  17% 682M/3.95G [00:04<00:22, 147MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  10% 377M/3.86G [00:04<00:42, 82.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  20% 703M/3.56G [00:04<00:18, 154MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  14% 535M/3.86G [00:04<00:29, 111MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  18% 703M/3.95G [00:04<00:21, 148MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  10% 388M/3.86G [00:04<00:42, 81.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  20% 724M/3.56G [00:04<00:18, 155MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  10% 398M/3.86G [00:04<00:41, 82.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  18% 724M/3.95G [00:04<00:21, 149MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  14% 556M/3.86G [00:05<00:29, 111MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  21% 744M/3.56G [00:05<00:18, 156MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  11% 409M/3.86G [00:04<00:41, 82.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  19% 744M/3.95G [00:05<00:21, 149MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  22% 765M/3.56G [00:05<00:17, 156MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  15% 577M/3.86G [00:05<00:29, 112MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  11% 419M/3.86G [00:05<00:41, 82.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  19% 765M/3.95G [00:05<00:21, 147MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  22% 786M/3.56G [00:05<00:17, 156MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  11% 430M/3.86G [00:05<00:41, 83.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  15% 598M/3.86G [00:05<00:29, 112MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  20% 786M/3.95G [00:05<00:21, 149MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  23% 807M/3.56G [00:05<00:17, 157MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  11% 440M/3.86G [00:05<00:41, 83.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  20% 807M/3.95G [00:05<00:20, 150MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  16% 619M/3.86G [00:05<00:28, 113MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  23% 828M/3.56G [00:05<00:17, 156MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  12% 451M/3.86G [00:05<00:41, 82.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  21% 828M/3.95G [00:05<00:21, 148MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  12% 461M/3.86G [00:05<00:40, 83.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  24% 849M/3.56G [00:05<00:17, 156MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  17% 640M/3.86G [00:05<00:28, 113MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  22% 849M/3.95G [00:05<00:20, 150MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  12% 472M/3.86G [00:05<00:40, 83.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  24% 870M/3.56G [00:05<00:17, 157MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  17% 661M/3.86G [00:05<00:28, 113MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  22% 870M/3.95G [00:05<00:20, 148MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  12% 482M/3.86G [00:05<00:40, 83.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  25% 891M/3.56G [00:05<00:16, 157MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  13% 493M/3.86G [00:06<00:41, 81.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  23% 891M/3.95G [00:06<00:20, 146MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  18% 682M/3.86G [00:06<00:30, 105MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  26% 912M/3.56G [00:06<00:19, 138MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  13% 503M/3.86G [00:06<00:40, 83.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  23% 912M/3.95G [00:06<00:20, 148MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  13% 514M/3.86G [00:06<00:40, 83.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  26% 933M/3.56G [00:06<00:20, 126MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  18% 703M/3.86G [00:06<00:31, 99.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  24% 933M/3.95G [00:06<00:21, 139MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  14% 524M/3.86G [00:06<00:40, 83.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  27% 954M/3.56G [00:06<00:19, 132MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  24% 954M/3.95G [00:06<00:21, 142MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  19% 724M/3.86G [00:06<00:30, 102MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  14% 535M/3.86G [00:06<00:40, 82.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  27% 975M/3.56G [00:06<00:18, 137MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  19% 734M/3.86G [00:06<00:34, 90.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  14% 545M/3.86G [00:06<00:44, 73.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  25% 975M/3.95G [00:06<00:25, 117MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  28% 996M/3.56G [00:06<00:21, 122MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  15% 566M/3.86G [00:06<00:38, 84.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  20% 755M/3.86G [00:06<00:33, 92.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  25% 996M/3.95G [00:06<00:25, 116MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  29% 1.02G/3.56G [00:07<00:20, 122MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  15% 577M/3.86G [00:07<00:38, 84.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  26% 1.02G/3.95G [00:07<00:23, 123MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  20% 776M/3.86G [00:07<00:31, 97.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  29% 1.04G/3.56G [00:07<00:23, 108MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  20% 786M/3.86G [00:07<00:32, 94.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  26% 1.04G/3.95G [00:07<00:25, 112MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  15% 587M/3.86G [00:07<00:52, 62.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  30% 1.06G/3.56G [00:07<00:22, 111MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  15% 598M/3.86G [00:07<00:47, 69.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  21% 807M/3.86G [00:07<00:31, 97.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  27% 1.06G/3.95G [00:07<00:25, 113MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  30% 1.08G/3.56G [00:07<00:20, 121MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  16% 619M/3.86G [00:07<00:35, 90.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  27% 1.08G/3.95G [00:07<00:23, 122MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  21% 828M/3.86G [00:07<00:29, 101MB/s] \u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  31% 1.10G/3.56G [00:07<00:19, 129MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  16% 629M/3.86G [00:07<00:36, 87.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  28% 1.10G/3.95G [00:07<00:25, 110MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  17% 640M/3.86G [00:07<00:38, 82.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  22% 849M/3.86G [00:07<00:31, 94.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  32% 1.12G/3.56G [00:07<00:20, 119MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  17% 650M/3.86G [00:07<00:38, 83.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  28% 1.12G/3.95G [00:08<00:23, 120MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  23% 870M/3.86G [00:08<00:30, 98.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  17% 661M/3.86G [00:08<00:37, 86.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  17% 671M/3.86G [00:08<00:37, 84.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  23% 891M/3.86G [00:08<00:28, 104MB/s] \u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  32% 1.14G/3.56G [00:08<00:27, 89.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  29% 1.14G/3.95G [00:08<00:27, 102MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  18% 682M/3.86G [00:08<00:37, 84.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  33% 1.16G/3.56G [00:08<00:23, 103MB/s] \u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  24% 912M/3.86G [00:08<00:27, 106MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  30% 1.16G/3.95G [00:08<00:26, 104MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  18% 692M/3.86G [00:08<00:37, 83.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  18% 703M/3.86G [00:08<00:37, 83.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  24% 933M/3.86G [00:08<00:27, 107MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  33% 1.18G/3.56G [00:08<00:25, 94.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  18% 713M/3.86G [00:08<00:37, 83.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  30% 1.18G/3.95G [00:08<00:31, 87.8MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  34% 1.21G/3.56G [00:08<00:21, 107MB/s] \u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  25% 954M/3.86G [00:08<00:26, 109MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  19% 724M/3.86G [00:10<03:33, 14.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  30% 1.20G/3.95G [00:10<02:02, 22.5MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  34% 1.23G/3.56G [00:10<01:26, 27.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  19% 744M/3.86G [00:10<02:02, 25.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  31% 1.22G/3.95G [00:11<01:26, 31.4MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  35% 1.25G/3.56G [00:11<01:06, 34.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  31% 1.23G/3.95G [00:11<01:15, 36.2MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  35% 1.26G/3.56G [00:11<00:58, 39.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  20% 765M/3.86G [00:11<01:30, 34.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  32% 1.25G/3.95G [00:11<00:56, 47.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  20% 786M/3.86G [00:11<01:03, 48.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  25% 975M/3.86G [00:11<02:06, 22.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  36% 1.28G/3.56G [00:11<00:46, 48.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  21% 807M/3.86G [00:11<00:47, 64.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  32% 1.27G/3.95G [00:11<00:45, 59.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  26% 986M/3.86G [00:11<01:48, 26.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  21% 828M/3.86G [00:11<00:37, 81.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  33% 1.29G/3.95G [00:11<00:35, 74.2MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  37% 1.30G/3.56G [00:11<00:37, 60.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  26% 996M/3.86G [00:11<01:35, 30.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  22% 849M/3.86G [00:11<00:30, 99.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  37% 1.31G/3.56G [00:11<00:34, 65.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  33% 1.31G/3.95G [00:11<00:31, 83.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  23% 870M/3.86G [00:11<00:28, 106MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  26% 1.01G/3.86G [00:11<01:25, 33.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  37% 1.33G/3.56G [00:11<00:28, 77.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  34% 1.33G/3.95G [00:12<00:28, 90.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  27% 1.03G/3.86G [00:12<01:03, 44.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  38% 1.35G/3.56G [00:12<00:26, 84.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  34% 1.35G/3.95G [00:12<00:25, 102MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  23% 891M/3.86G [00:12<00:35, 84.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  35% 1.37G/3.95G [00:12<00:22, 114MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  39% 1.37G/3.56G [00:12<00:23, 91.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  27% 1.04G/3.86G [00:12<01:02, 45.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  24% 912M/3.86G [00:12<00:30, 96.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  35% 1.39G/3.95G [00:12<00:20, 122MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  39% 1.39G/3.56G [00:12<00:20, 105MB/s] \u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  27% 1.06G/3.86G [00:12<00:47, 59.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  40% 1.42G/3.56G [00:12<00:18, 117MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  36% 1.42G/3.95G [00:12<00:20, 125MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  24% 933M/3.86G [00:12<00:31, 92.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  28% 1.08G/3.86G [00:12<00:39, 70.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  40% 1.44G/3.56G [00:12<00:16, 127MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  36% 1.44G/3.95G [00:12<00:19, 131MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  37% 1.46G/3.95G [00:12<00:18, 135MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  25% 954M/3.86G [00:12<00:32, 89.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  37% 1.48G/3.95G [00:13<00:17, 139MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  28% 1.09G/3.86G [00:13<00:48, 57.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  41% 1.46G/3.56G [00:13<00:22, 91.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  28% 1.10G/3.86G [00:13<00:43, 63.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  25% 975M/3.86G [00:13<00:33, 87.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  38% 1.50G/3.95G [00:13<00:17, 141MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  42% 1.48G/3.56G [00:13<00:19, 104MB/s] \u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  26% 986M/3.86G [00:13<00:33, 86.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  29% 1.12G/3.86G [00:13<00:35, 76.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  39% 1.52G/3.95G [00:13<00:17, 139MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  42% 1.50G/3.56G [00:13<00:18, 113MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  26% 996M/3.86G [00:13<00:33, 85.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  39% 1.54G/3.95G [00:13<00:17, 139MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  30% 1.14G/3.86G [00:13<00:31, 86.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  26% 1.01G/3.86G [00:13<00:33, 85.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  43% 1.52G/3.56G [00:13<00:17, 119MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  40% 1.56G/3.95G [00:13<00:17, 139MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  26% 1.02G/3.86G [00:13<00:33, 84.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  43% 1.54G/3.56G [00:13<00:15, 128MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  30% 1.16G/3.86G [00:13<00:28, 93.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  40% 1.58G/3.95G [00:13<00:16, 143MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  27% 1.03G/3.86G [00:13<00:33, 84.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  44% 1.56G/3.56G [00:13<00:14, 136MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  31% 1.18G/3.86G [00:13<00:27, 99.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  41% 1.60G/3.95G [00:13<00:16, 145MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  27% 1.04G/3.86G [00:13<00:33, 83.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  45% 1.58G/3.56G [00:13<00:13, 142MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  27% 1.05G/3.86G [00:13<00:33, 83.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  41% 1.63G/3.95G [00:14<00:16, 144MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  45% 1.60G/3.56G [00:14<00:13, 146MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  31% 1.21G/3.86G [00:14<00:26, 101MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  27% 1.06G/3.86G [00:14<00:33, 83.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  42% 1.65G/3.95G [00:14<00:16, 143MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  46% 1.63G/3.56G [00:14<00:12, 149MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  32% 1.23G/3.86G [00:14<00:25, 104MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  28% 1.07G/3.86G [00:14<00:33, 82.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  46% 1.65G/3.56G [00:14<00:12, 151MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  42% 1.67G/3.95G [00:14<00:15, 144MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  28% 1.08G/3.86G [00:14<00:33, 81.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  32% 1.25G/3.86G [00:14<00:24, 106MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  47% 1.67G/3.56G [00:14<00:12, 152MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  43% 1.69G/3.95G [00:14<00:15, 143MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  28% 1.09G/3.86G [00:14<00:33, 83.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  47% 1.69G/3.56G [00:14<00:12, 152MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  33% 1.27G/3.86G [00:14<00:23, 109MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  43% 1.71G/3.95G [00:14<00:15, 142MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  28% 1.10G/3.86G [00:14<00:33, 83.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  48% 1.71G/3.56G [00:14<00:12, 153MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  44% 1.73G/3.95G [00:14<00:15, 144MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  29% 1.11G/3.86G [00:14<00:33, 82.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  33% 1.29G/3.86G [00:14<00:23, 109MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  49% 1.73G/3.56G [00:14<00:12, 151MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  29% 1.12G/3.86G [00:14<00:32, 83.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  44% 1.75G/3.95G [00:14<00:15, 145MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  34% 1.31G/3.86G [00:15<00:23, 111MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  49% 1.75G/3.56G [00:15<00:11, 153MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  29% 1.13G/3.86G [00:14<00:32, 83.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  45% 1.77G/3.95G [00:15<00:15, 145MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  50% 1.77G/3.56G [00:15<00:11, 153MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  30% 1.14G/3.86G [00:15<00:32, 82.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  34% 1.33G/3.86G [00:15<00:22, 112MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  45% 1.79G/3.95G [00:15<00:14, 147MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  50% 1.79G/3.56G [00:15<00:11, 155MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  30% 1.15G/3.86G [00:15<00:32, 82.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  46% 1.81G/3.95G [00:15<00:14, 147MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  35% 1.35G/3.86G [00:15<00:22, 112MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  30% 1.16G/3.86G [00:15<00:32, 82.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  51% 1.81G/3.56G [00:15<00:11, 153MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  47% 1.84G/3.95G [00:15<00:14, 144MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  30% 1.17G/3.86G [00:15<00:32, 82.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  36% 1.37G/3.86G [00:15<00:22, 112MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  52% 1.84G/3.56G [00:15<00:11, 154MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  47% 1.86G/3.95G [00:15<00:14, 145MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  52% 1.86G/3.56G [00:15<00:11, 154MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  31% 1.18G/3.86G [00:15<00:35, 76.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  36% 1.39G/3.86G [00:15<00:21, 113MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  48% 1.88G/3.95G [00:15<00:14, 145MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  53% 1.88G/3.56G [00:15<00:10, 155MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  31% 1.20G/3.86G [00:15<00:34, 77.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  48% 1.90G/3.95G [00:15<00:14, 145MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  37% 1.42G/3.86G [00:15<00:21, 111MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  31% 1.21G/3.86G [00:15<00:33, 78.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  53% 1.90G/3.56G [00:16<00:10, 152MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  49% 1.92G/3.95G [00:16<00:14, 144MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  31% 1.22G/3.86G [00:16<00:33, 79.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  54% 1.92G/3.56G [00:16<00:10, 153MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  37% 1.44G/3.86G [00:16<00:21, 112MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  32% 1.23G/3.86G [00:16<00:32, 80.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  55% 1.94G/3.56G [00:16<00:10, 153MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  38% 1.46G/3.86G [00:16<00:21, 112MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  32% 1.24G/3.86G [00:16<00:32, 81.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  55% 1.96G/3.56G [00:16<00:10, 154MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  49% 1.94G/3.95G [00:16<00:20, 97.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  32% 1.25G/3.86G [00:16<00:31, 82.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  38% 1.48G/3.86G [00:16<00:21, 113MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  56% 1.98G/3.56G [00:16<00:10, 155MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  50% 1.96G/3.95G [00:16<00:18, 110MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  33% 1.26G/3.86G [00:16<00:31, 81.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  56% 2.00G/3.56G [00:16<00:10, 155MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  39% 1.50G/3.86G [00:16<00:20, 113MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  33% 1.27G/3.86G [00:16<00:31, 82.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  50% 1.98G/3.95G [00:16<00:16, 119MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  57% 2.02G/3.56G [00:16<00:09, 156MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  39% 1.52G/3.86G [00:16<00:20, 113MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  33% 1.28G/3.86G [00:16<00:31, 81.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  51% 2.00G/3.95G [00:16<00:15, 126MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  57% 2.04G/3.56G [00:16<00:09, 156MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  33% 1.29G/3.86G [00:16<00:31, 83.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  51% 2.02G/3.95G [00:17<00:14, 132MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  40% 1.54G/3.86G [00:17<00:20, 113MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  58% 2.07G/3.56G [00:17<00:09, 155MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  34% 1.30G/3.86G [00:17<00:30, 83.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  52% 2.04G/3.95G [00:17<00:13, 137MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  59% 2.09G/3.56G [00:17<00:09, 154MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  40% 1.56G/3.86G [00:17<00:20, 112MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  34% 1.31G/3.86G [00:17<00:30, 82.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  52% 2.07G/3.95G [00:17<00:13, 139MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  59% 2.11G/3.56G [00:17<00:09, 152MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  34% 1.32G/3.86G [00:17<00:30, 83.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  41% 1.58G/3.86G [00:17<00:20, 112MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  53% 2.09G/3.95G [00:17<00:13, 142MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  60% 2.13G/3.56G [00:17<00:09, 153MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  34% 1.33G/3.86G [00:17<00:30, 83.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  53% 2.11G/3.95G [00:17<00:12, 146MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  60% 2.15G/3.56G [00:17<00:09, 153MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  35% 1.34G/3.86G [00:17<00:30, 83.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  42% 1.60G/3.86G [00:17<00:20, 111MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  54% 2.13G/3.95G [00:17<00:12, 146MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  35% 1.35G/3.86G [00:17<00:30, 82.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  61% 2.17G/3.56G [00:17<00:09, 150MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  42% 1.63G/3.86G [00:17<00:19, 112MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  54% 2.15G/3.95G [00:17<00:12, 146MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  35% 1.36G/3.86G [00:17<00:30, 82.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  62% 2.19G/3.56G [00:17<00:09, 147MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  43% 1.65G/3.86G [00:18<00:19, 111MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  36% 1.37G/3.86G [00:17<00:30, 81.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  55% 2.17G/3.95G [00:18<00:12, 146MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  62% 2.21G/3.56G [00:18<00:09, 148MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  36% 1.38G/3.86G [00:18<00:29, 83.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  56% 2.19G/3.95G [00:18<00:12, 146MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  43% 1.67G/3.86G [00:18<00:19, 111MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  63% 2.23G/3.56G [00:18<00:08, 147MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  36% 1.39G/3.86G [00:18<00:29, 83.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  56% 2.21G/3.95G [00:18<00:11, 145MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  63% 2.25G/3.56G [00:18<00:08, 148MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  44% 1.69G/3.86G [00:18<00:19, 111MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  36% 1.41G/3.86G [00:18<00:29, 83.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  57% 2.23G/3.95G [00:18<00:11, 146MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  64% 2.28G/3.56G [00:18<00:08, 149MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  37% 1.42G/3.86G [00:18<00:29, 83.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  44% 1.71G/3.86G [00:18<00:19, 112MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  57% 2.25G/3.95G [00:18<00:11, 147MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  65% 2.30G/3.56G [00:18<00:08, 151MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  37% 1.43G/3.86G [00:18<00:29, 83.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  58% 2.28G/3.95G [00:18<00:11, 148MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  65% 2.32G/3.56G [00:18<00:08, 152MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  45% 1.73G/3.86G [00:18<00:19, 112MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  37% 1.44G/3.86G [00:18<00:29, 81.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  58% 2.30G/3.95G [00:18<00:11, 149MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  66% 2.34G/3.56G [00:18<00:07, 154MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  37% 1.45G/3.86G [00:18<00:29, 83.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  45% 1.75G/3.86G [00:18<00:18, 113MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  59% 2.32G/3.95G [00:19<00:10, 150MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  66% 2.36G/3.56G [00:19<00:07, 155MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  38% 1.46G/3.86G [00:18<00:29, 81.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  59% 2.34G/3.95G [00:19<00:10, 151MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  67% 2.38G/3.56G [00:19<00:07, 155MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  60% 2.36G/3.95G [00:19<00:10, 153MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  38% 1.47G/3.86G [00:19<00:40, 59.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  46% 1.77G/3.86G [00:19<00:25, 80.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  68% 2.40G/3.56G [00:19<00:08, 129MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  60% 2.38G/3.95G [00:19<00:10, 153MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  46% 1.78G/3.86G [00:19<00:26, 79.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  39% 1.49G/3.86G [00:19<00:33, 71.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  61% 2.40G/3.95G [00:19<00:10, 154MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  46% 1.79G/3.86G [00:19<00:25, 80.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  39% 1.50G/3.86G [00:19<00:33, 71.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  61% 2.42G/3.95G [00:19<00:09, 153MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  68% 2.42G/3.56G [00:19<00:12, 93.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  39% 1.51G/3.86G [00:19<00:31, 74.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  62% 2.44G/3.95G [00:19<00:09, 154MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  47% 1.80G/3.86G [00:19<00:28, 72.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  39% 1.52G/3.86G [00:19<00:31, 73.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  47% 1.81G/3.86G [00:20<00:28, 72.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  69% 2.44G/3.56G [00:20<00:12, 88.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  62% 2.46G/3.95G [00:20<00:10, 136MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  47% 1.82G/3.86G [00:20<00:27, 74.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  69% 2.45G/3.56G [00:20<00:12, 90.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  40% 1.54G/3.86G [00:20<00:28, 81.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  69% 2.46G/3.56G [00:20<00:12, 90.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  63% 2.49G/3.95G [00:20<00:12, 115MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  40% 1.55G/3.86G [00:20<00:27, 85.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  48% 1.85G/3.86G [00:20<00:24, 83.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  70% 2.47G/3.56G [00:20<00:11, 92.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  48% 1.86G/3.86G [00:20<00:23, 83.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  70% 2.49G/3.56G [00:20<00:12, 89.0MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  64% 2.51G/3.95G [00:20<00:13, 107MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  41% 1.57G/3.86G [00:20<00:27, 84.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  48% 1.87G/3.86G [00:20<00:22, 87.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  70% 2.50G/3.56G [00:20<00:11, 89.6MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  70% 2.51G/3.56G [00:20<00:17, 59.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  49% 1.88G/3.86G [00:21<00:40, 49.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  64% 2.53G/3.95G [00:21<00:19, 71.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  41% 1.58G/3.86G [00:20<00:46, 49.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  71% 2.53G/3.56G [00:21<00:12, 81.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  65% 2.55G/3.95G [00:21<00:16, 85.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  42% 1.60G/3.86G [00:21<00:32, 68.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  72% 2.55G/3.56G [00:21<00:10, 99.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  49% 1.90G/3.86G [00:21<00:30, 65.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  42% 1.63G/3.86G [00:21<00:25, 89.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  65% 2.57G/3.95G [00:21<00:14, 97.6MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  72% 2.57G/3.56G [00:21<00:08, 113MB/s] \u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  50% 1.92G/3.86G [00:21<00:25, 77.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  43% 1.65G/3.86G [00:21<00:23, 96.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  73% 2.59G/3.56G [00:21<00:07, 123MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  50% 1.93G/3.86G [00:21<00:24, 78.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  66% 2.59G/3.95G [00:21<00:13, 97.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  50% 1.94G/3.86G [00:21<00:27, 70.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  73% 2.61G/3.56G [00:21<00:09, 104MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  43% 1.67G/3.86G [00:21<00:24, 88.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  66% 2.61G/3.95G [00:21<00:14, 89.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  51% 1.96G/3.86G [00:21<00:22, 83.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  74% 2.63G/3.56G [00:21<00:08, 104MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  44% 1.69G/3.86G [00:21<00:23, 93.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  67% 2.63G/3.95G [00:22<00:14, 93.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  51% 1.97G/3.86G [00:22<00:22, 83.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  67% 2.64G/3.95G [00:22<00:13, 93.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  51% 1.98G/3.86G [00:22<00:23, 80.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  75% 2.65G/3.56G [00:22<00:09, 95.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  44% 1.71G/3.86G [00:22<00:24, 87.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  67% 2.65G/3.95G [00:22<00:14, 89.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  52% 1.99G/3.86G [00:22<00:22, 83.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  75% 2.66G/3.56G [00:22<00:09, 93.4MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  44% 1.72G/3.86G [00:22<00:26, 80.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  68% 2.66G/3.95G [00:22<00:16, 78.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  52% 2.00G/3.86G [00:22<00:24, 75.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  75% 2.67G/3.56G [00:22<00:10, 85.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  45% 1.73G/3.86G [00:22<00:28, 74.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  68% 2.67G/3.95G [00:22<00:17, 73.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  52% 2.01G/3.86G [00:22<00:27, 67.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  75% 2.68G/3.56G [00:22<00:11, 76.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  45% 1.74G/3.86G [00:22<00:31, 68.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  68% 2.68G/3.95G [00:22<00:19, 64.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  52% 2.02G/3.86G [00:22<00:28, 64.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  76% 2.69G/3.56G [00:22<00:12, 66.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  45% 1.75G/3.86G [00:22<00:30, 68.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  68% 2.69G/3.95G [00:23<00:19, 64.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  53% 2.03G/3.86G [00:23<00:30, 60.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  76% 2.71G/3.56G [00:23<00:12, 65.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  46% 1.76G/3.86G [00:23<00:33, 63.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  69% 2.71G/3.95G [00:23<00:20, 60.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  53% 2.04G/3.86G [00:23<00:29, 61.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  76% 2.72G/3.56G [00:23<00:13, 61.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  46% 1.77G/3.86G [00:23<00:34, 60.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  69% 2.72G/3.95G [00:23<00:20, 59.9MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  77% 2.73G/3.56G [00:23<00:12, 65.6MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  53% 2.06G/3.86G [00:23<00:30, 58.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  46% 1.78G/3.86G [00:23<00:30, 67.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  69% 2.73G/3.95G [00:23<00:18, 67.7MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  77% 2.74G/3.56G [00:23<00:11, 73.1MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  77% 2.75G/3.56G [00:23<00:10, 79.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  54% 2.08G/3.86G [00:23<00:25, 70.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  47% 1.80G/3.86G [00:23<00:26, 77.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  70% 2.75G/3.95G [00:23<00:14, 81.1MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  78% 2.76G/3.56G [00:23<00:11, 68.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  54% 2.09G/3.86G [00:23<00:27, 63.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  47% 1.81G/3.86G [00:26<02:52, 11.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  54% 2.10G/3.86G [00:28<04:01, 7.32MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  70% 2.76G/3.95G [00:28<02:35, 7.66MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  47% 1.82G/3.86G [00:28<03:53, 8.74MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  70% 2.77G/3.95G [00:29<01:59, 9.88MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  55% 2.11G/3.86G [00:29<03:03, 9.56MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  78% 2.77G/3.56G [00:29<02:06, 6.25MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  47% 1.84G/3.86G [00:29<02:57, 11.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  70% 2.78G/3.95G [00:29<01:31, 12.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  55% 2.12G/3.86G [00:29<02:19, 12.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  78% 2.79G/3.56G [00:29<01:08, 11.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  48% 1.85G/3.86G [00:29<02:20, 14.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  79% 2.80G/3.56G [00:29<00:54, 13.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  71% 2.79G/3.95G [00:29<01:11, 16.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  55% 2.13G/3.86G [00:29<01:48, 16.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  48% 1.86G/3.86G [00:29<01:51, 18.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  71% 2.80G/3.95G [00:29<00:57, 20.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  55% 2.14G/3.86G [00:29<01:26, 20.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  79% 2.81G/3.56G [00:29<00:43, 17.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  48% 1.87G/3.86G [00:29<01:30, 22.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  79% 2.82G/3.56G [00:29<00:34, 21.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  56% 2.15G/3.86G [00:29<01:10, 24.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  71% 2.81G/3.95G [00:30<00:51, 21.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  49% 1.88G/3.86G [00:29<01:16, 26.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  80% 2.83G/3.56G [00:30<00:29, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  56% 2.16G/3.86G [00:30<01:00, 28.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  71% 2.82G/3.95G [00:30<00:39, 28.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  49% 1.89G/3.86G [00:30<01:08, 28.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  72% 2.83G/3.95G [00:30<00:35, 31.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  56% 2.17G/3.86G [00:30<00:55, 30.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  80% 2.84G/3.56G [00:30<00:26, 27.2MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  49% 1.90G/3.86G [00:30<01:07, 29.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  56% 2.18G/3.86G [00:30<00:55, 30.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  72% 2.84G/3.95G [00:30<00:37, 29.7MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  80% 2.85G/3.56G [00:30<00:26, 26.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  49% 1.91G/3.86G [00:31<01:14, 26.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  57% 2.19G/3.86G [00:31<01:02, 26.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  72% 2.85G/3.95G [00:31<00:41, 26.4MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  80% 2.86G/3.56G [00:31<00:28, 24.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  50% 1.92G/3.86G [00:31<01:12, 26.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  57% 2.20G/3.86G [00:31<00:59, 27.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  73% 2.86G/3.95G [00:31<00:38, 28.4MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  81% 2.87G/3.56G [00:31<00:25, 27.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  50% 1.93G/3.86G [00:31<00:57, 33.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  73% 2.87G/3.95G [00:31<00:29, 36.3MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  81% 2.88G/3.56G [00:31<00:19, 34.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  57% 2.21G/3.86G [00:31<00:47, 34.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  50% 1.95G/3.86G [00:31<00:35, 53.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  82% 2.90G/3.56G [00:31<00:12, 53.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  73% 2.89G/3.95G [00:31<00:19, 54.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  51% 1.97G/3.86G [00:31<00:25, 74.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  82% 2.93G/3.56G [00:31<00:08, 71.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  74% 2.92G/3.95G [00:32<00:14, 72.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  52% 1.99G/3.86G [00:31<00:19, 93.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  58% 2.23G/3.86G [00:32<00:39, 41.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  83% 2.95G/3.56G [00:32<00:06, 89.0MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  74% 2.94G/3.95G [00:32<00:11, 89.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  52% 2.01G/3.86G [00:32<00:16, 111MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  83% 2.97G/3.56G [00:32<00:05, 104MB/s] \u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  75% 2.96G/3.95G [00:32<00:09, 103MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  53% 2.03G/3.86G [00:32<00:15, 122MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  58% 2.25G/3.86G [00:32<00:29, 54.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  84% 2.99G/3.56G [00:32<00:04, 117MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  75% 2.98G/3.95G [00:32<00:08, 114MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  53% 2.06G/3.86G [00:32<00:14, 125MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  59% 2.28G/3.86G [00:32<00:23, 67.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  85% 3.01G/3.56G [00:32<00:04, 127MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  76% 3.00G/3.95G [00:32<00:07, 122MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  54% 2.08G/3.86G [00:32<00:14, 122MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  59% 2.30G/3.86G [00:32<00:19, 79.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  85% 3.03G/3.56G [00:32<00:04, 120MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  77% 3.02G/3.95G [00:32<00:07, 118MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  60% 2.32G/3.86G [00:32<00:17, 88.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  86% 3.05G/3.56G [00:32<00:04, 111MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  60% 2.33G/3.86G [00:32<00:17, 89.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  77% 3.04G/3.95G [00:32<00:08, 111MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  54% 2.10G/3.86G [00:32<00:20, 84.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  61% 2.35G/3.86G [00:33<00:15, 96.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  55% 2.12G/3.86G [00:33<00:17, 101MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  86% 3.07G/3.56G [00:33<00:04, 104MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  78% 3.06G/3.95G [00:33<00:08, 103MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  61% 2.37G/3.86G [00:33<00:14, 101MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  55% 2.14G/3.86G [00:33<00:16, 105MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  87% 3.09G/3.56G [00:33<00:04, 94.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  78% 3.08G/3.95G [00:33<00:09, 92.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  62% 2.39G/3.86G [00:33<00:13, 106MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  87% 3.10G/3.56G [00:33<00:05, 87.8MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  56% 2.16G/3.86G [00:33<00:18, 91.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  78% 3.09G/3.95G [00:33<00:09, 85.8MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  88% 3.11G/3.56G [00:33<00:05, 83.6MB/s]\u001b[A\n",
            "model-00004-of-00004.safetensors:  88% 3.12G/3.56G [00:33<00:05, 81.9MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  56% 2.18G/3.86G [00:33<00:19, 84.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  62% 2.41G/3.86G [00:33<00:19, 75.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  88% 3.14G/3.56G [00:34<00:05, 79.0MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  79% 3.10G/3.95G [00:34<00:14, 60.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  57% 2.19G/3.86G [00:33<00:20, 82.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  63% 2.42G/3.86G [00:34<00:19, 75.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  88% 3.15G/3.56G [00:34<00:05, 77.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  79% 3.11G/3.95G [00:34<00:13, 62.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  57% 2.20G/3.86G [00:34<00:20, 80.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  63% 2.43G/3.86G [00:34<00:19, 75.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  79% 3.12G/3.95G [00:34<00:12, 68.0MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  89% 3.16G/3.56G [00:34<00:05, 77.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  57% 2.21G/3.86G [00:34<00:20, 79.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  63% 2.44G/3.86G [00:34<00:18, 75.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  79% 3.14G/3.95G [00:34<00:11, 69.9MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  89% 3.17G/3.56G [00:34<00:04, 79.3MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  58% 2.22G/3.86G [00:34<00:21, 77.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  63% 2.45G/3.86G [00:34<00:19, 74.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  89% 3.18G/3.56G [00:34<00:04, 77.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  80% 3.15G/3.95G [00:34<00:11, 69.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  58% 2.23G/3.86G [00:34<00:21, 74.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  64% 2.46G/3.86G [00:34<00:19, 71.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  90% 3.19G/3.56G [00:34<00:05, 73.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  80% 3.16G/3.95G [00:34<00:11, 68.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  58% 2.24G/3.86G [00:34<00:21, 76.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  64% 2.47G/3.86G [00:34<00:19, 72.1MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  90% 3.20G/3.56G [00:34<00:04, 76.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  80% 3.17G/3.95G [00:34<00:10, 72.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  58% 2.25G/3.86G [00:34<00:19, 80.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  64% 2.49G/3.86G [00:34<00:18, 75.3MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  90% 3.21G/3.56G [00:34<00:04, 80.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  81% 3.18G/3.95G [00:34<00:10, 75.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  59% 2.26G/3.86G [00:34<00:19, 83.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  91% 3.22G/3.56G [00:35<00:03, 85.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  81% 3.19G/3.95G [00:35<00:09, 81.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  65% 2.51G/3.86G [00:35<00:15, 88.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  59% 2.29G/3.86G [00:35<00:16, 93.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  91% 3.24G/3.56G [00:35<00:03, 99.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  81% 3.21G/3.95G [00:35<00:07, 95.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  65% 2.53G/3.86G [00:35<00:14, 95.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  60% 2.31G/3.86G [00:35<00:15, 101MB/s] \u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  92% 3.26G/3.56G [00:35<00:02, 105MB/s] \u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  82% 3.23G/3.95G [00:35<00:07, 102MB/s] \u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  66% 2.55G/3.86G [00:35<00:13, 100MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  60% 2.33G/3.86G [00:35<00:14, 107MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  92% 3.28G/3.56G [00:35<00:02, 109MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  82% 3.25G/3.95G [00:35<00:06, 107MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  66% 2.57G/3.86G [00:35<00:12, 104MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  61% 2.35G/3.86G [00:35<00:14, 103MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  93% 3.30G/3.56G [00:35<00:02, 112MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  83% 3.27G/3.95G [00:35<00:06, 111MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  67% 2.59G/3.86G [00:35<00:11, 107MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  61% 2.36G/3.86G [00:35<00:15, 98.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  93% 3.32G/3.56G [00:35<00:02, 116MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  83% 3.29G/3.95G [00:36<00:05, 111MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  61% 2.37G/3.86G [00:35<00:15, 94.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  68% 2.61G/3.86G [00:36<00:11, 108MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  94% 3.34G/3.56G [00:36<00:01, 114MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  62% 2.38G/3.86G [00:36<00:16, 91.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  84% 3.31G/3.95G [00:36<00:05, 109MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  68% 2.63G/3.86G [00:36<00:11, 110MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  62% 2.39G/3.86G [00:36<00:16, 89.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  95% 3.37G/3.56G [00:36<00:01, 107MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  62% 2.40G/3.86G [00:36<00:16, 86.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  85% 3.33G/3.95G [00:36<00:06, 99.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  69% 2.65G/3.86G [00:36<00:11, 107MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  62% 2.41G/3.86G [00:36<00:16, 86.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  85% 3.34G/3.95G [00:36<00:05, 101MB/s] \u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  95% 3.39G/3.56G [00:36<00:01, 102MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  63% 2.42G/3.86G [00:36<00:16, 85.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  85% 3.36G/3.95G [00:36<00:06, 94.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  69% 2.67G/3.86G [00:36<00:11, 99.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  96% 3.40G/3.56G [00:36<00:01, 94.5MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  63% 2.43G/3.86G [00:36<00:16, 84.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  85% 3.37G/3.95G [00:36<00:06, 90.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  69% 2.68G/3.86G [00:36<00:12, 93.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  96% 3.41G/3.56G [00:36<00:01, 92.0MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  63% 2.44G/3.86G [00:36<00:16, 84.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  86% 3.38G/3.95G [00:36<00:06, 88.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  70% 2.69G/3.86G [00:36<00:13, 89.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  96% 3.42G/3.56G [00:37<00:01, 85.7MB/s]\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  63% 2.45G/3.86G [00:36<00:17, 82.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  86% 3.39G/3.95G [00:37<00:06, 87.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  70% 2.71G/3.86G [00:37<00:13, 86.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  96% 3.43G/3.56G [00:37<00:01, 87.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  86% 3.40G/3.95G [00:37<00:06, 85.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  64% 2.46G/3.86G [00:37<00:17, 77.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  70% 2.72G/3.86G [00:37<00:13, 85.8MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  97% 3.44G/3.56G [00:37<00:01, 86.0MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  86% 3.41G/3.95G [00:37<00:06, 83.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  64% 2.47G/3.86G [00:37<00:17, 78.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  71% 2.73G/3.86G [00:37<00:13, 82.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  97% 3.45G/3.56G [00:37<00:01, 83.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  87% 3.42G/3.95G [00:37<00:06, 84.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  64% 2.49G/3.86G [00:37<00:17, 79.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  71% 2.74G/3.86G [00:37<00:13, 83.6MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  97% 3.46G/3.56G [00:37<00:01, 84.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  87% 3.43G/3.95G [00:37<00:06, 85.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  65% 2.50G/3.86G [00:37<00:17, 80.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  71% 2.75G/3.86G [00:37<00:12, 86.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  98% 3.47G/3.56G [00:37<00:01, 84.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  87% 3.44G/3.95G [00:37<00:05, 88.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  65% 2.51G/3.86G [00:37<00:16, 80.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  98% 3.48G/3.56G [00:37<00:00, 88.1MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  72% 2.77G/3.86G [00:37<00:11, 91.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  65% 2.52G/3.86G [00:37<00:16, 81.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  88% 3.46G/3.95G [00:37<00:04, 97.1MB/s]\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  98% 3.50G/3.56G [00:37<00:00, 102MB/s] \u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  65% 2.53G/3.86G [00:37<00:16, 81.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  88% 3.48G/3.95G [00:38<00:04, 105MB/s] \u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors:  99% 3.52G/3.56G [00:38<00:00, 119MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  72% 2.79G/3.86G [00:38<00:11, 93.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  66% 2.54G/3.86G [00:38<00:16, 82.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  89% 3.50G/3.95G [00:38<00:03, 111MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  66% 2.55G/3.86G [00:38<00:15, 82.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors: 100% 3.54G/3.56G [00:38<00:00, 112MB/s]\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  73% 2.81G/3.86G [00:38<00:10, 96.2MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  66% 2.56G/3.86G [00:38<00:15, 82.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "model-00004-of-00004.safetensors: 100% 3.56G/3.56G [00:38<00:00, 92.7MB/s]\n",
            "\n",
            "\n",
            "model-00001-of-00004.safetensors:  89% 3.52G/3.95G [00:38<00:03, 113MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  66% 2.57G/3.86G [00:38<00:15, 82.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  73% 2.83G/3.86G [00:38<00:10, 94.4MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  90% 3.54G/3.95G [00:38<00:03, 123MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  67% 2.58G/3.86G [00:38<00:15, 82.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  90% 3.57G/3.95G [00:38<00:02, 131MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  74% 2.85G/3.86G [00:38<00:10, 99.9MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  67% 2.59G/3.86G [00:38<00:15, 82.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  91% 3.59G/3.95G [00:38<00:02, 136MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  67% 2.60G/3.86G [00:38<00:15, 82.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  74% 2.87G/3.86G [00:38<00:09, 104MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  91% 3.61G/3.95G [00:38<00:02, 141MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  75% 2.88G/3.86G [00:38<00:09, 103MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  68% 2.61G/3.86G [00:38<00:15, 82.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  92% 3.63G/3.95G [00:39<00:02, 143MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  68% 2.62G/3.86G [00:39<00:15, 82.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  75% 2.90G/3.86G [00:39<00:08, 107MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  92% 3.65G/3.95G [00:39<00:02, 142MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  68% 2.63G/3.86G [00:39<00:14, 82.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  76% 2.93G/3.86G [00:39<00:08, 106MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  68% 2.64G/3.86G [00:39<00:14, 82.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  93% 3.67G/3.95G [00:39<00:01, 142MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  69% 2.65G/3.86G [00:39<00:14, 82.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  94% 3.69G/3.95G [00:39<00:01, 145MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  76% 2.95G/3.86G [00:39<00:08, 107MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  69% 2.66G/3.86G [00:39<00:14, 82.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  94% 3.71G/3.95G [00:39<00:01, 147MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  77% 2.97G/3.86G [00:39<00:08, 109MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  69% 2.67G/3.86G [00:39<00:14, 82.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  95% 3.73G/3.95G [00:39<00:01, 148MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  69% 2.68G/3.86G [00:39<00:14, 82.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  77% 2.99G/3.86G [00:39<00:07, 111MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  70% 2.69G/3.86G [00:39<00:14, 82.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  95% 3.75G/3.95G [00:40<00:01, 130MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  78% 3.01G/3.86G [00:40<00:07, 112MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  70% 2.71G/3.86G [00:40<00:13, 82.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  96% 3.77G/3.95G [00:40<00:01, 129MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  70% 2.72G/3.86G [00:40<00:13, 82.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  78% 3.03G/3.86G [00:40<00:07, 113MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  96% 3.80G/3.95G [00:40<00:01, 135MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  71% 2.73G/3.86G [00:40<00:13, 82.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  97% 3.82G/3.95G [00:40<00:00, 138MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  79% 3.05G/3.86G [00:40<00:07, 114MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  71% 2.74G/3.86G [00:40<00:13, 82.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  97% 3.84G/3.95G [00:40<00:00, 142MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  71% 2.75G/3.86G [00:40<00:13, 82.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  79% 3.07G/3.86G [00:40<00:06, 115MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  98% 3.86G/3.95G [00:40<00:00, 144MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  71% 2.76G/3.86G [00:40<00:13, 81.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  80% 3.09G/3.86G [00:40<00:06, 114MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  98% 3.88G/3.95G [00:40<00:00, 145MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  72% 2.77G/3.86G [00:40<00:13, 83.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  81% 3.11G/3.86G [00:40<00:06, 115MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  72% 2.78G/3.86G [00:40<00:13, 83.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  99% 3.90G/3.95G [00:41<00:00, 140MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  72% 2.79G/3.86G [00:41<00:13, 82.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors:  99% 3.92G/3.95G [00:41<00:00, 142MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  81% 3.14G/3.86G [00:41<00:06, 113MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  72% 2.80G/3.86G [00:41<00:13, 78.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00004.safetensors: 100% 3.95G/3.95G [00:41<00:00, 95.4MB/s]\n",
            "Fetching 4 files:  25% 1/4 [00:41<02:04, 41.52s/it]\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  82% 3.16G/3.86G [00:41<00:06, 113MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  73% 2.81G/3.86G [00:41<00:13, 79.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  73% 2.82G/3.86G [00:41<00:13, 79.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  82% 3.18G/3.86G [00:41<00:06, 114MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  73% 2.83G/3.86G [00:41<00:12, 80.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  83% 3.20G/3.86G [00:41<00:05, 114MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  74% 2.84G/3.86G [00:41<00:12, 80.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  83% 3.22G/3.86G [00:41<00:05, 115MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  74% 2.85G/3.86G [00:41<00:12, 80.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  74% 2.86G/3.86G [00:41<00:12, 81.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  84% 3.24G/3.86G [00:42<00:05, 115MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  74% 2.87G/3.86G [00:42<00:12, 81.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  84% 3.26G/3.86G [00:42<00:05, 116MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  75% 2.88G/3.86G [00:42<00:11, 82.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  75% 2.89G/3.86G [00:42<00:11, 82.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  85% 3.28G/3.86G [00:42<00:05, 115MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  75% 2.90G/3.86G [00:42<00:11, 82.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  85% 3.30G/3.86G [00:42<00:04, 114MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  75% 2.92G/3.86G [00:42<00:11, 82.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  76% 2.93G/3.86G [00:42<00:11, 82.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  86% 3.32G/3.86G [00:42<00:04, 114MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  76% 2.94G/3.86G [00:42<00:11, 82.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  76% 2.95G/3.86G [00:42<00:11, 82.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  77% 2.96G/3.86G [00:43<00:10, 82.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  87% 3.34G/3.86G [00:43<00:06, 84.0MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  77% 2.97G/3.86G [00:43<00:10, 82.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  87% 3.37G/3.86G [00:43<00:05, 91.5MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  77% 2.98G/3.86G [00:43<00:10, 82.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  77% 2.99G/3.86G [00:43<00:10, 82.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  88% 3.39G/3.86G [00:43<00:04, 97.7MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  78% 3.00G/3.86G [00:43<00:10, 82.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  88% 3.41G/3.86G [00:43<00:04, 103MB/s] \u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  78% 3.01G/3.86G [00:43<00:10, 82.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  89% 3.43G/3.86G [00:43<00:04, 107MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  78% 3.02G/3.86G [00:43<00:10, 82.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  89% 3.45G/3.86G [00:44<00:03, 109MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  78% 3.03G/3.86G [00:44<00:15, 55.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  90% 3.47G/3.86G [00:44<00:03, 112MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  79% 3.04G/3.86G [00:44<00:13, 60.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  90% 3.49G/3.86G [00:44<00:03, 113MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  79% 3.07G/3.86G [00:44<00:08, 92.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  91% 3.51G/3.86G [00:44<00:03, 115MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  80% 3.08G/3.86G [00:44<00:08, 89.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  91% 3.53G/3.86G [00:44<00:02, 115MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  80% 3.09G/3.86G [00:44<00:10, 74.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  92% 3.55G/3.86G [00:45<00:02, 116MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  81% 3.11G/3.86G [00:45<00:07, 94.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  93% 3.58G/3.86G [00:45<00:02, 115MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  81% 3.12G/3.86G [00:45<00:08, 92.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  81% 3.14G/3.86G [00:45<00:08, 88.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  93% 3.60G/3.86G [00:45<00:02, 115MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  81% 3.15G/3.86G [00:45<00:08, 86.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  94% 3.62G/3.86G [00:45<00:02, 116MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  82% 3.16G/3.86G [00:45<00:08, 84.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  94% 3.64G/3.86G [00:45<00:01, 116MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  82% 3.17G/3.86G [00:45<00:08, 83.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  82% 3.18G/3.86G [00:45<00:08, 81.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  95% 3.66G/3.86G [00:45<00:01, 116MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  82% 3.19G/3.86G [00:45<00:08, 81.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  95% 3.68G/3.86G [00:46<00:01, 115MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  83% 3.20G/3.86G [00:46<00:08, 81.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  83% 3.21G/3.86G [00:46<00:08, 82.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  96% 3.70G/3.86G [00:46<00:01, 115MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  83% 3.22G/3.86G [00:46<00:07, 82.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  96% 3.72G/3.86G [00:46<00:01, 116MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  84% 3.23G/3.86G [00:46<00:07, 82.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  84% 3.24G/3.86G [00:46<00:07, 82.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  97% 3.74G/3.86G [00:46<00:01, 116MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  84% 3.25G/3.86G [00:46<00:07, 81.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  97% 3.76G/3.86G [00:46<00:00, 116MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  84% 3.26G/3.86G [00:46<00:07, 78.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  98% 3.79G/3.86G [00:47<00:00, 117MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  85% 3.27G/3.86G [00:46<00:07, 83.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  85% 3.28G/3.86G [00:47<00:06, 83.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  98% 3.81G/3.86G [00:47<00:00, 117MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  85% 3.29G/3.86G [00:47<00:06, 83.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors:  99% 3.83G/3.86G [00:47<00:00, 117MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  85% 3.30G/3.86G [00:47<00:06, 83.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  86% 3.31G/3.86G [00:47<00:06, 83.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors: 100% 3.85G/3.86G [00:47<00:00, 116MB/s]\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  86% 3.32G/3.86G [00:47<00:06, 82.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "model-00002-of-00004.safetensors: 100% 3.86G/3.86G [00:47<00:00, 81.0MB/s]\n",
            "Fetching 4 files:  50% 2/4 [00:47<00:41, 20.85s/it]\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  86% 3.33G/3.86G [00:47<00:06, 82.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  87% 3.34G/3.86G [00:47<00:06, 82.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  87% 3.36G/3.86G [00:47<00:06, 82.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  87% 3.37G/3.86G [00:48<00:06, 82.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  87% 3.38G/3.86G [00:48<00:05, 82.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  88% 3.39G/3.86G [00:48<00:05, 83.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  88% 3.40G/3.86G [00:48<00:05, 82.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  88% 3.41G/3.86G [00:48<00:05, 82.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  88% 3.42G/3.86G [00:48<00:05, 82.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  89% 3.43G/3.86G [00:48<00:05, 82.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  89% 3.44G/3.86G [00:48<00:05, 82.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  89% 3.45G/3.86G [00:49<00:05, 81.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  90% 3.46G/3.86G [00:49<00:04, 82.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  90% 3.47G/3.86G [00:49<00:04, 82.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  90% 3.48G/3.86G [00:49<00:04, 81.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  90% 3.49G/3.86G [00:49<00:04, 80.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  91% 3.50G/3.86G [00:49<00:04, 80.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  91% 3.51G/3.86G [00:49<00:04, 80.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  91% 3.52G/3.86G [00:50<00:04, 81.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  91% 3.53G/3.86G [00:50<00:04, 81.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  92% 3.54G/3.86G [00:50<00:03, 80.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  92% 3.55G/3.86G [00:50<00:03, 81.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  92% 3.57G/3.86G [00:50<00:03, 80.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  93% 3.58G/3.86G [00:50<00:03, 81.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  93% 3.59G/3.86G [00:50<00:03, 81.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  93% 3.60G/3.86G [00:50<00:03, 82.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  93% 3.61G/3.86G [00:51<00:03, 82.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  94% 3.62G/3.86G [00:51<00:03, 82.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  94% 3.63G/3.86G [00:51<00:02, 82.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  94% 3.64G/3.86G [00:51<00:02, 82.6MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  94% 3.65G/3.86G [00:51<00:02, 81.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  95% 3.66G/3.86G [00:51<00:02, 82.8MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  95% 3.67G/3.86G [00:51<00:02, 83.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  95% 3.68G/3.86G [00:51<00:02, 81.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  96% 3.69G/3.86G [00:52<00:02, 83.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  96% 3.70G/3.86G [00:52<00:01, 82.5MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  96% 3.71G/3.86G [00:52<00:01, 83.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  96% 3.72G/3.86G [00:52<00:01, 83.0MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  97% 3.73G/3.86G [00:52<00:01, 82.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  97% 3.74G/3.86G [00:52<00:01, 82.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  97% 3.75G/3.86G [00:52<00:01, 82.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  97% 3.76G/3.86G [00:52<00:01, 82.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  98% 3.77G/3.86G [00:53<00:01, 82.4MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  98% 3.79G/3.86G [00:53<00:00, 82.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  98% 3.80G/3.86G [00:53<00:00, 82.3MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  98% 3.81G/3.86G [00:53<00:00, 81.7MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  99% 3.82G/3.86G [00:53<00:00, 82.1MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  99% 3.83G/3.86G [00:53<00:00, 82.2MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors:  99% 3.84G/3.86G [00:53<00:00, 81.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors: 100% 3.85G/3.86G [00:53<00:00, 81.9MB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "model-00003-of-00004.safetensors: 100% 3.86G/3.86G [00:54<00:00, 71.3MB/s]\n",
            "Fetching 4 files: 100% 4/4 [00:54<00:00, 13.62s/it]\n",
            "[INFO|modeling_utils.py:2170] 2025-04-14 07:12:20,010 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1139] 2025-04-14 07:12:20,013 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "[WARNING|logging.py:329] 2025-04-14 07:12:20,052 >> Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
            "Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.15s/it]\n",
            "[INFO|modeling_utils.py:4987] 2025-04-14 07:12:24,703 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4995] 2025-04-14 07:12:24,703 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at lordjia/Qwen2-Cantonese-7B-Instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
            "generation_config.json: 100% 243/243 [00:00<00:00, 2.14MB/s]\n",
            "[INFO|configuration_utils.py:1094] 2025-04-14 07:12:24,818 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/generation_config.json\n",
            "[INFO|configuration_utils.py:1139] 2025-04-14 07:12:24,818 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    151645,\n",
            "    151643\n",
            "  ],\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"repetition_penalty\": 1.05,\n",
            "  \"temperature\": 0.7,\n",
            "  \"top_k\": 20,\n",
            "  \"top_p\": 0.8\n",
            "}\n",
            "\n",
            "[INFO|2025-04-14 07:12:24] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
            "[INFO|2025-04-14 07:12:24] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-04-14 07:12:24] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
            "[INFO|2025-04-14 07:12:24] llamafactory.model.adapter:143 >> Fine-tuning method: DoRA\n",
            "[INFO|2025-04-14 07:12:24] llamafactory.model.model_utils.misc:143 >> Found linear modules: q_proj,v_proj,o_proj,down_proj,gate_proj,k_proj,up_proj\n",
            "[INFO|2025-04-14 07:12:26] llamafactory.model.loader:143 >> trainable params: 21,575,680 || all params: 7,637,192,192 || trainable%: 0.2825\n",
            "[INFO|trainer.py:748] 2025-04-14 07:12:27,068 >> Using auto half precision backend\n",
            "[WARNING|trainer.py:783] 2025-04-14 07:12:27,069 >> No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "[INFO|trainer.py:2409] 2025-04-14 07:12:27,478 >> ***** Running training *****\n",
            "[INFO|trainer.py:2410] 2025-04-14 07:12:27,478 >>   Num examples = 11,289\n",
            "[INFO|trainer.py:2411] 2025-04-14 07:12:27,478 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2412] 2025-04-14 07:12:27,479 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:2415] 2025-04-14 07:12:27,479 >>   Total train batch size (w. parallel, distributed & accumulation) = 256\n",
            "[INFO|trainer.py:2416] 2025-04-14 07:12:27,479 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2417] 2025-04-14 07:12:27,479 >>   Total optimization steps = 132\n",
            "[INFO|trainer.py:2418] 2025-04-14 07:12:27,483 >>   Number of trainable parameters = 21,575,680\n",
            "  4% 5/132 [03:01<1:16:29, 36.14s/it][INFO|2025-04-14 07:15:28] llamafactory.train.callbacks:143 >> {'loss': 1.8813, 'learning_rate': 4.9823e-05, 'epoch': 0.11, 'throughput': 1808.49}\n",
            "{'loss': 1.8813, 'grad_norm': 2.7640585899353027, 'learning_rate': 4.982319711683221e-05, 'epoch': 0.11, 'num_input_tokens_seen': 327680}\n",
            "  8% 10/132 [06:01<1:13:21, 36.08s/it][INFO|2025-04-14 07:18:29] llamafactory.train.callbacks:143 >> {'loss': 0.6460, 'learning_rate': 4.9295e-05, 'epoch': 0.23, 'throughput': 1812.67}\n",
            "{'loss': 0.646, 'grad_norm': 2.8224217891693115, 'learning_rate': 4.929528920808854e-05, 'epoch': 0.23, 'num_input_tokens_seen': 655360}\n",
            " 11% 15/132 [09:01<1:10:22, 36.09s/it][INFO|2025-04-14 07:21:29] llamafactory.train.callbacks:143 >> {'loss': 0.4609, 'learning_rate': 4.8424e-05, 'epoch': 0.34, 'throughput': 1813.77}\n",
            "{'loss': 0.4609, 'grad_norm': 3.2628350257873535, 'learning_rate': 4.842374312499405e-05, 'epoch': 0.34, 'num_input_tokens_seen': 983040}\n",
            " 15% 20/132 [12:02<1:07:21, 36.09s/it][INFO|2025-04-14 07:24:29] llamafactory.train.callbacks:143 >> {'loss': 0.2208, 'learning_rate': 4.7221e-05, 'epoch': 0.45, 'throughput': 1814.36}\n",
            "{'loss': 0.2208, 'grad_norm': 2.0283913612365723, 'learning_rate': 4.722088621637309e-05, 'epoch': 0.45, 'num_input_tokens_seen': 1310720}\n",
            " 19% 25/132 [15:02<1:04:20, 36.08s/it][INFO|2025-04-14 07:27:30] llamafactory.train.callbacks:143 >> {'loss': 0.1771, 'learning_rate': 4.5704e-05, 'epoch': 0.57, 'throughput': 1814.78}\n",
            "{'loss': 0.1771, 'grad_norm': 2.470806360244751, 'learning_rate': 4.570373196778427e-05, 'epoch': 0.57, 'num_input_tokens_seen': 1638400}\n",
            " 23% 30/132 [18:03<1:01:20, 36.08s/it][INFO|2025-04-14 07:30:30] llamafactory.train.callbacks:143 >> {'loss': 0.1387, 'learning_rate': 4.3894e-05, 'epoch': 0.68, 'throughput': 1815.05}\n",
            "{'loss': 0.1387, 'grad_norm': 3.281022071838379, 'learning_rate': 4.389373935885646e-05, 'epoch': 0.68, 'num_input_tokens_seen': 1966080}\n",
            " 27% 35/132 [21:03<58:19, 36.08s/it][INFO|2025-04-14 07:33:31] llamafactory.train.callbacks:143 >> {'loss': 0.1108, 'learning_rate': 4.1817e-05, 'epoch': 0.79, 'throughput': 1815.26}\n",
            "{'loss': 0.1108, 'grad_norm': 1.0010223388671875, 'learning_rate': 4.181650934253132e-05, 'epoch': 0.79, 'num_input_tokens_seen': 2293760}\n",
            " 30% 40/132 [24:03<55:18, 36.07s/it][INFO|2025-04-14 07:36:31] llamafactory.train.callbacks:143 >> {'loss': 0.1184, 'learning_rate': 3.9501e-05, 'epoch': 0.91, 'throughput': 1815.45}\n",
            "{'loss': 0.1184, 'grad_norm': 1.2688794136047363, 'learning_rate': 3.9501422739279956e-05, 'epoch': 0.91, 'num_input_tokens_seen': 2621440}\n",
            " 34% 45/132 [26:31<38:12, 26.35s/it][INFO|2025-04-14 07:38:59] llamafactory.train.callbacks:143 >> {'loss': 0.1057, 'learning_rate': 3.6981e-05, 'epoch': 1.00, 'throughput': 1815.41}\n",
            "{'loss': 0.1057, 'grad_norm': 4.66291618347168, 'learning_rate': 3.6981224668001424e-05, 'epoch': 1.0, 'num_input_tokens_seen': 2889984}\n",
            " 38% 50/132 [29:32<47:04, 34.44s/it][INFO|2025-04-14 07:41:59] llamafactory.train.callbacks:143 >> {'loss': 0.0882, 'learning_rate': 3.4292e-05, 'epoch': 1.11, 'throughput': 1815.53}\n",
            "{'loss': 0.0882, 'grad_norm': 0.3550863265991211, 'learning_rate': 3.4291561391508185e-05, 'epoch': 1.11, 'num_input_tokens_seen': 3217664}\n",
            " 42% 55/132 [32:32<45:56, 35.80s/it][INFO|2025-04-14 07:45:00] llamafactory.train.callbacks:143 >> {'loss': 0.0918, 'learning_rate': 3.1470e-05, 'epoch': 1.23, 'throughput': 1815.62}\n",
            "{'loss': 0.0918, 'grad_norm': 0.6538587212562561, 'learning_rate': 3.147047612756302e-05, 'epoch': 1.23, 'num_input_tokens_seen': 3545344}\n",
            " 45% 60/132 [35:33<43:14, 36.04s/it][INFO|2025-04-14 07:48:00] llamafactory.train.callbacks:143 >> {'loss': 0.0911, 'learning_rate': 2.8558e-05, 'epoch': 1.34, 'throughput': 1815.65}\n",
            "{'loss': 0.0911, 'grad_norm': 0.7042384147644043, 'learning_rate': 2.8557870956832132e-05, 'epoch': 1.34, 'num_input_tokens_seen': 3873024}\n",
            " 49% 65/132 [38:33<40:17, 36.08s/it][INFO|2025-04-14 07:51:01] llamafactory.train.callbacks:143 >> {'loss': 0.0838, 'learning_rate': 2.5595e-05, 'epoch': 1.45, 'throughput': 1815.67}\n",
            "{'loss': 0.0838, 'grad_norm': 1.7436611652374268, 'learning_rate': 2.5594942438652688e-05, 'epoch': 1.45, 'num_input_tokens_seen': 4200704}\n",
            " 53% 70/132 [41:33<37:16, 36.07s/it][INFO|2025-04-14 07:54:01] llamafactory.train.callbacks:143 >> {'loss': 0.0764, 'learning_rate': 2.2624e-05, 'epoch': 1.57, 'throughput': 1815.69}\n",
            "{'loss': 0.0764, 'grad_norm': 0.8280196785926819, 'learning_rate': 2.2623598917395438e-05, 'epoch': 1.57, 'num_input_tokens_seen': 4528128}\n",
            " 57% 75/132 [44:34<34:16, 36.08s/it][INFO|2025-04-14 07:57:01] llamafactory.train.callbacks:143 >> {'loss': 0.0854, 'learning_rate': 1.9686e-05, 'epoch': 1.68, 'throughput': 1815.74}\n",
            "{'loss': 0.0854, 'grad_norm': 0.6205782890319824, 'learning_rate': 1.9685867761175584e-05, 'epoch': 1.68, 'num_input_tokens_seen': 4855808}\n",
            " 61% 80/132 [47:34<31:15, 36.07s/it][INFO|2025-04-14 08:00:02] llamafactory.train.callbacks:143 >> {'loss': 0.0869, 'learning_rate': 1.6823e-05, 'epoch': 1.79, 'throughput': 1815.80}\n",
            "{'loss': 0.0869, 'grad_norm': 0.9628000855445862, 'learning_rate': 1.682330091706446e-05, 'epoch': 1.79, 'num_input_tokens_seen': 5183488}\n",
            " 64% 85/132 [50:35<28:15, 36.07s/it][INFO|2025-04-14 08:03:02] llamafactory.train.callbacks:143 >> {'loss': 0.0802, 'learning_rate': 1.4076e-05, 'epoch': 1.91, 'throughput': 1815.86}\n",
            "{'loss': 0.0802, 'grad_norm': 0.4409404993057251, 'learning_rate': 1.4076387190766017e-05, 'epoch': 1.91, 'num_input_tokens_seen': 5511168}\n",
            " 68% 90/132 [53:02<18:26, 26.35s/it][INFO|2025-04-14 08:05:30] llamafactory.train.callbacks:143 >> {'loss': 0.0767, 'learning_rate': 1.1484e-05, 'epoch': 2.00, 'throughput': 1815.82}\n",
            "{'loss': 0.0767, 'grad_norm': 0.8611986637115479, 'learning_rate': 1.148397956361007e-05, 'epoch': 2.0, 'num_input_tokens_seen': 5779712}\n",
            " 72% 95/132 [56:03<21:14, 34.44s/it][INFO|2025-04-14 08:08:30] llamafactory.train.callbacks:143 >> {'loss': 0.0753, 'learning_rate': 9.0827e-06, 'epoch': 2.11, 'throughput': 1815.87}\n",
            "{'loss': 0.0753, 'grad_norm': 0.569953203201294, 'learning_rate': 9.082745647022797e-06, 'epoch': 2.11, 'num_input_tokens_seen': 6107392}\n",
            " 76% 100/132 [59:03<19:05, 35.80s/it][INFO|2025-04-14 08:11:31] llamafactory.train.callbacks:143 >> {'loss': 0.0744, 'learning_rate': 6.9066e-06, 'epoch': 2.23, 'throughput': 1815.92}\n",
            "{'loss': 0.0744, 'grad_norm': 0.7752261757850647, 'learning_rate': 6.906649047373246e-06, 'epoch': 2.23, 'num_input_tokens_seen': 6435072}\n",
            " 76% 100/132 [59:03<19:05, 35.80s/it][INFO|trainer.py:4289] 2025-04-14 08:11:31,195 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4291] 2025-04-14 08:11:31,195 >>   Num examples = 595\n",
            "[INFO|trainer.py:4294] 2025-04-14 08:11:31,195 >>   Batch size = 32\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:01<00:12,  1.35it/s]\u001b[A\n",
            " 16% 3/19 [00:02<00:16,  1.05s/it]\u001b[A\n",
            " 21% 4/19 [00:04<00:18,  1.21s/it]\u001b[A\n",
            " 26% 5/19 [00:05<00:18,  1.30s/it]\u001b[A\n",
            " 32% 6/19 [00:07<00:17,  1.36s/it]\u001b[A\n",
            " 37% 7/19 [00:08<00:16,  1.40s/it]\u001b[A\n",
            " 42% 8/19 [00:10<00:15,  1.42s/it]\u001b[A\n",
            " 47% 9/19 [00:11<00:14,  1.44s/it]\u001b[A\n",
            " 53% 10/19 [00:13<00:13,  1.45s/it]\u001b[A\n",
            " 58% 11/19 [00:14<00:11,  1.46s/it]\u001b[A\n",
            " 63% 12/19 [00:16<00:10,  1.47s/it]\u001b[A\n",
            " 68% 13/19 [00:17<00:08,  1.47s/it]\u001b[A\n",
            " 74% 14/19 [00:19<00:07,  1.47s/it]\u001b[A\n",
            " 79% 15/19 [00:20<00:05,  1.47s/it]\u001b[A\n",
            " 84% 16/19 [00:22<00:04,  1.47s/it]\u001b[A\n",
            " 89% 17/19 [00:23<00:02,  1.48s/it]\u001b[A\n",
            " 95% 18/19 [00:25<00:01,  1.48s/it]\u001b[A\n",
            "100% 19/19 [00:26<00:00,  1.33s/it]\u001b[A\n",
            "{'eval_loss': 0.09337212145328522, 'eval_runtime': 27.6058, 'eval_samples_per_second': 21.553, 'eval_steps_per_second': 0.688, 'epoch': 2.23, 'num_input_tokens_seen': 6435072}\n",
            "\n",
            " 76% 100/132 [59:31<19:05, 35.80s/it]\n",
            "                                   \u001b[A[INFO|trainer.py:3966] 2025-04-14 08:11:58,801 >> Saving model checkpoint to saves/Custom/lora/train_2025-04-14-07-09-23/checkpoint-100\n",
            "[INFO|configuration_utils.py:699] 2025-04-14 08:11:59,117 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-04-14 08:11:59,118 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3584,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 18944,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 28,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 4,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 131072,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.50.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 152064\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-14 08:11:59,321 >> tokenizer config file saved in saves/Custom/lora/train_2025-04-14-07-09-23/checkpoint-100/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-14 08:11:59,321 >> Special tokens file saved in saves/Custom/lora/train_2025-04-14-07-09-23/checkpoint-100/special_tokens_map.json\n",
            " 80% 105/132 [1:02:32<17:08, 38.10s/it][INFO|2025-04-14 08:15:00] llamafactory.train.callbacks:143 >> {'loss': 0.0714, 'learning_rate': 4.9865e-06, 'epoch': 2.34, 'throughput': 1802.05}\n",
            "{'loss': 0.0714, 'grad_norm': 0.37949469685554504, 'learning_rate': 4.986468976890993e-06, 'epoch': 2.34, 'num_input_tokens_seen': 6762752}\n",
            " 83% 110/132 [1:05:33<13:21, 36.42s/it][INFO|2025-04-14 08:18:00] llamafactory.train.callbacks:143 >> {'loss': 0.0733, 'learning_rate': 3.3494e-06, 'epoch': 2.45, 'throughput': 1802.70}\n",
            "{'loss': 0.0733, 'grad_norm': 0.42053738236427307, 'learning_rate': 3.3493649053890326e-06, 'epoch': 2.45, 'num_input_tokens_seen': 7090432}\n",
            " 87% 115/132 [1:08:33<10:14, 36.13s/it][INFO|2025-04-14 08:21:01] llamafactory.train.callbacks:143 >> {'loss': 0.0764, 'learning_rate': 2.0185e-06, 'epoch': 2.57, 'throughput': 1803.31}\n",
            "{'loss': 0.0764, 'grad_norm': 0.6681663990020752, 'learning_rate': 2.0184924104583613e-06, 'epoch': 2.57, 'num_input_tokens_seen': 7418112}\n",
            " 91% 120/132 [1:11:33<07:12, 36.08s/it][INFO|2025-04-14 08:24:01] llamafactory.train.callbacks:143 >> {'loss': 0.0782, 'learning_rate': 1.0127e-06, 'epoch': 2.68, 'throughput': 1803.88}\n",
            "{'loss': 0.0782, 'grad_norm': 0.3759320080280304, 'learning_rate': 1.0126756596375686e-06, 'epoch': 2.68, 'num_input_tokens_seen': 7745792}\n",
            " 95% 125/132 [1:14:34<04:12, 36.08s/it][INFO|2025-04-14 08:27:01] llamafactory.train.callbacks:143 >> {'loss': 0.0746, 'learning_rate': 3.4614e-07, 'epoch': 2.79, 'throughput': 1804.38}\n",
            "{'loss': 0.0746, 'grad_norm': 0.6606363654136658, 'learning_rate': 3.4614115704533767e-07, 'epoch': 2.79, 'num_input_tokens_seen': 8073472}\n",
            " 98% 130/132 [1:17:34<01:12, 36.08s/it][INFO|2025-04-14 08:30:02] llamafactory.train.callbacks:143 >> {'loss': 0.0770, 'learning_rate': 2.8317e-08, 'epoch': 2.91, 'throughput': 1804.85}\n",
            "{'loss': 0.077, 'grad_norm': 0.32873448729515076, 'learning_rate': 2.831652042480093e-08, 'epoch': 2.91, 'num_input_tokens_seen': 8401152}\n",
            "100% 132/132 [1:18:46<00:00, 36.08s/it][INFO|trainer.py:3966] 2025-04-14 08:31:14,410 >> Saving model checkpoint to saves/Custom/lora/train_2025-04-14-07-09-23/checkpoint-132\n",
            "[INFO|configuration_utils.py:699] 2025-04-14 08:31:15,004 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-04-14 08:31:15,005 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3584,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 18944,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 28,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 4,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 131072,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.50.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 152064\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-14 08:31:15,172 >> tokenizer config file saved in saves/Custom/lora/train_2025-04-14-07-09-23/checkpoint-132/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-14 08:31:15,172 >> Special tokens file saved in saves/Custom/lora/train_2025-04-14-07-09-23/checkpoint-132/special_tokens_map.json\n",
            "[INFO|trainer.py:2665] 2025-04-14 08:31:15,716 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 4728.2325, 'train_samples_per_second': 7.163, 'train_steps_per_second': 0.028, 'train_loss': 0.1986924640157006, 'epoch': 2.95, 'num_input_tokens_seen': 8532224}\n",
            "100% 132/132 [1:18:48<00:00, 35.82s/it]\n",
            "[INFO|trainer.py:3966] 2025-04-14 08:31:15,717 >> Saving model checkpoint to saves/Custom/lora/train_2025-04-14-07-09-23\n",
            "[INFO|configuration_utils.py:699] 2025-04-14 08:31:15,845 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-04-14 08:31:15,846 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3584,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 18944,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 28,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 4,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 131072,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.50.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 152064\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-14 08:31:16,005 >> tokenizer config file saved in saves/Custom/lora/train_2025-04-14-07-09-23/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-14 08:31:16,005 >> Special tokens file saved in saves/Custom/lora/train_2025-04-14-07-09-23/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =      2.9518\n",
            "  num_input_tokens_seen    =     8532224\n",
            "  total_flos               = 338138238GF\n",
            "  train_loss               =      0.1987\n",
            "  train_runtime            =  1:18:48.23\n",
            "  train_samples_per_second =       7.163\n",
            "  train_steps_per_second   =       0.028\n",
            "Figure saved at: saves/Custom/lora/train_2025-04-14-07-09-23/training_loss.png\n",
            "Figure saved at: saves/Custom/lora/train_2025-04-14-07-09-23/training_eval_loss.png\n",
            "[WARNING|2025-04-14 08:31:16] llamafactory.extras.ploting:148 >> No metric eval_accuracy to plot.\n",
            "[INFO|trainer.py:4289] 2025-04-14 08:31:16,455 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4291] 2025-04-14 08:31:16,455 >>   Num examples = 595\n",
            "[INFO|trainer.py:4294] 2025-04-14 08:31:16,455 >>   Batch size = 32\n",
            "100% 19/19 [00:26<00:00,  1.37s/it]\n",
            "***** eval metrics *****\n",
            "  epoch                   =     2.9518\n",
            "  eval_loss               =     0.0903\n",
            "  eval_runtime            = 0:00:27.65\n",
            "  eval_samples_per_second =     21.518\n",
            "  eval_steps_per_second   =      0.687\n",
            "  num_input_tokens_seen   =    8532224\n",
            "[INFO|modelcard.py:449] 2025-04-14 08:31:44,107 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
            "2025-04-14 08:34:08.203098: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744619648.224677   24854 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744619648.231320   24854 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "[INFO|2025-04-14 08:34:14] llamafactory.hparams.parser:379 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 08:34:14,753 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 08:34:14,754 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 08:34:14,754 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 08:34:14,754 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 08:34:14,754 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 08:34:14,754 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 08:34:14,754 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2323] 2025-04-14 08:34:15,101 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:699] 2025-04-14 08:34:15,301 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-04-14 08:34:15,302 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3584,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 18944,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 28,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 4,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 131072,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.50.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 152064\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 08:34:15,348 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 08:34:15,348 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 08:34:15,348 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 08:34:15,348 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 08:34:15,349 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 08:34:15,349 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 08:34:15,349 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2323] 2025-04-14 08:34:15,693 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|2025-04-14 08:34:15] llamafactory.data.loader:143 >> Loading dataset Openrice_test_alpaca_with_emojis.json...\n",
            "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
            "Generating train split: 6128 examples [00:00, 76010.42 examples/s]\n",
            "Converting format of dataset (num_proc=16): 100% 6128/6128 [00:00<00:00, 20810.77 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16): 100% 6128/6128 [00:03<00:00, 1691.43 examples/s]\n",
            "eval example:\n",
            "input_ids:\n",
            "[33975, 25, 220, 56568, 101909, 101897, 104034, 100623, 48692, 100168, 3837, 73670, 101042, 108704, 31196, 90395, 100345, 102285, 16744, 105278, 70538, 17714, 117585, 109955, 31905, 1773, 103929, 88802, 20412, 104857, 102086, 31196, 62926, 60610, 31196, 100409, 113195, 104405, 1773, 45181, 87752, 104405, 15946, 50404, 5122, 16, 13, 1036, 106557, 33590, 17, 34047, 103276, 55807, 91680, 99553, 104405, 9370, 29991, 100622, 66017, 3837, 16530, 99553, 108593, 9370, 104136, 8997, 102242, 101443, 114016, 55338, 118952, 103102, 108703, 26939, 44, 60204, 99416, 102171, 99416, 102171, 24562, 11, 30534, 99738, 105925, 115009, 60726, 102810, 99593, 99542, 99765, 42468, 100294, 99512, 11, 103532, 101570, 104091, 52801, 99375, 99662, 101655, 11, 77288, 99765, 42468, 100294, 69442, 32757, 101461, 82647, 104686, 11, 63431, 100480, 116433, 30709, 34187, 11, 99271, 52853, 120110, 108871, 105793, 104334, 11, 106053, 105925, 20412, 99369, 119101, 11, 77288, 104302, 105161, 99416, 102171, 34187, 151645, 198, 71703, 25]\n",
            "inputs:\n",
            "Human: 你是一个经过训练的人工智能，可以分析文本输入，并根据上下文将其分类为最合适的情感类型。你的任务是仔细评估输入并确定输入属于哪种情绪。从以下情绪中选择：1. “正面”，2.“负面”。只提供情绪的名称作为输出，不提供额外的解释。\n",
            "今日帶齊所有考試範圍到M記温書温書前,要令肚子乖乖先買了一個巨無霸餐,薯條依然好味香脆,但巨無霸份量則少了很多,整體縮小了,百物昴貴嗎好了,雖然肚子是半飽,但也要開始温書了<|im_end|>\n",
            "Assistant:\n",
            "label_ids:\n",
            "[106557, 151645, 198]\n",
            "labels:\n",
            "正面<|im_end|>\n",
            "\n",
            "[INFO|configuration_utils.py:699] 2025-04-14 08:34:20,768 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-04-14 08:34:20,769 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3584,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 18944,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 28,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 4,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 131072,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.50.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 152064\n",
            "}\n",
            "\n",
            "[INFO|2025-04-14 08:34:20] llamafactory.model.model_utils.kv_cache:143 >> KV cache is enabled for faster generation.\n",
            "[INFO|modeling_utils.py:1154] 2025-04-14 08:34:20,831 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:2170] 2025-04-14 08:34:20,832 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1139] 2025-04-14 08:34:20,833 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645\n",
            "}\n",
            "\n",
            "[WARNING|logging.py:329] 2025-04-14 08:34:20,835 >> Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
            "Loading checkpoint shards: 100% 4/4 [00:05<00:00,  1.35s/it]\n",
            "[INFO|modeling_utils.py:4987] 2025-04-14 08:34:26,259 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4995] 2025-04-14 08:34:26,259 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at lordjia/Qwen2-Cantonese-7B-Instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:1094] 2025-04-14 08:34:27,373 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/generation_config.json\n",
            "[INFO|configuration_utils.py:1139] 2025-04-14 08:34:27,373 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    151645,\n",
            "    151643\n",
            "  ],\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"repetition_penalty\": 1.05,\n",
            "  \"temperature\": 0.7,\n",
            "  \"top_k\": 20,\n",
            "  \"top_p\": 0.8\n",
            "}\n",
            "\n",
            "[INFO|2025-04-14 08:34:27] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-04-14 08:34:28] llamafactory.model.adapter:143 >> Merged 1 adapter(s).\n",
            "[INFO|2025-04-14 08:34:28] llamafactory.model.adapter:143 >> Loaded adapter(s): saves/Custom/lora/train_2025-04-14-07-09-23\n",
            "[INFO|2025-04-14 08:34:28] llamafactory.model.loader:143 >> all params: 7,615,616,512\n",
            "[WARNING|2025-04-14 08:34:28] llamafactory.train.sft.workflow:154 >> Batch generation can be very slow. Consider using `scripts/vllm_infer.py` instead.\n",
            "[INFO|trainer.py:4289] 2025-04-14 08:34:28,716 >> \n",
            "***** Running Prediction *****\n",
            "[INFO|trainer.py:4291] 2025-04-14 08:34:28,717 >>   Num examples = 6128\n",
            "[INFO|trainer.py:4294] 2025-04-14 08:34:28,717 >>   Batch size = 32\n",
            "100% 192/192 [02:01<00:00,  1.82it/s]Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 0.669 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "[INFO|integration_utils.py:831] 2025-04-14 08:36:33,620 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcharlotte-geng111\u001b[0m (\u001b[33mcharlotte-geng111-wenzhou-kean-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/LLaMA-Factory/wandb/run-20250414_083743-g39ogznl\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msaves/Custom/lora/eval_2025-04-14-07-09-23\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/charlotte-geng111-wenzhou-kean-university/llamafactory\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/charlotte-geng111-wenzhou-kean-university/llamafactory/runs/g39ogznl\u001b[0m\n",
            "100% 192/192 [03:14<00:00,  1.01s/it]\n",
            "***** predict metrics *****\n",
            "  predict_bleu-4                 =    34.2577\n",
            "  predict_model_preparation_time =     0.0049\n",
            "  predict_rouge-1                =    89.9804\n",
            "  predict_rouge-2                =        0.0\n",
            "  predict_rouge-l                =    89.9804\n",
            "  predict_runtime                = 0:02:04.90\n",
            "  predict_samples_per_second     =     49.062\n",
            "  predict_steps_per_second       =      1.537\n",
            "[INFO|2025-04-14 08:37:44] llamafactory.train.sft.trainer:143 >> Saving prediction results to saves/Custom/lora/eval_2025-04-14-07-09-23/generated_predictions.jsonl\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33msaves/Custom/lora/eval_2025-04-14-07-09-23\u001b[0m at: \u001b[34mhttps://wandb.ai/charlotte-geng111-wenzhou-kean-university/llamafactory/runs/g39ogznl\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250414_083743-g39ogznl/logs\u001b[0m\n",
            "2025-04-14 08:39:58.442922: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744619998.464224   26558 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744619998.470760   26558 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "[INFO|2025-04-14 08:40:04] llamafactory.hparams.parser:379 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.bfloat16\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 08:40:05,013 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 08:40:05,013 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 08:40:05,013 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 08:40:05,013 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 08:40:05,013 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 08:40:05,013 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 08:40:05,013 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2323] 2025-04-14 08:40:05,367 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:699] 2025-04-14 08:40:05,543 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-04-14 08:40:05,544 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3584,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 18944,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 28,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 4,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 131072,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.50.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 152064\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 08:40:05,621 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 08:40:05,621 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 08:40:05,621 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 08:40:05,621 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 08:40:05,621 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 08:40:05,621 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 08:40:05,621 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2323] 2025-04-14 08:40:05,974 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|2025-04-14 08:40:06] llamafactory.data.loader:143 >> Loading dataset Openrice_train_alpaca_with_emojis_texts.json...\n",
            "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
            "Generating train split: 11884 examples [00:00, 78751.59 examples/s]\n",
            "Converting format of dataset (num_proc=16): 100% 11884/11884 [00:00<00:00, 31234.36 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16): 100% 11884/11884 [00:04<00:00, 2965.68 examples/s]\n",
            "training example:\n",
            "input_ids:\n",
            "[33975, 25, 220, 56568, 101909, 101897, 104034, 100623, 48692, 100168, 3837, 73670, 101042, 108704, 31196, 90395, 100345, 102285, 16744, 105278, 70538, 17714, 117585, 109955, 31905, 1773, 103929, 88802, 20412, 104857, 102086, 31196, 62926, 60610, 31196, 100409, 113195, 104405, 1773, 45181, 87752, 104405, 15946, 50404, 5122, 16, 13, 1036, 106557, 33590, 17, 34047, 103276, 55807, 91680, 99553, 104405, 9370, 29991, 100622, 66017, 3837, 16530, 99553, 108593, 9370, 104136, 8997, 104064, 61443, 99450, 63379, 112512, 113049, 100039, 99627, 109379, 100220, 107380, 23, 99699, 106372, 20, 19, 15, 107380, 99512, 69249, 69103, 44636, 111199, 68536, 106898, 112109, 109692, 99165, 104838, 101885, 101229, 101677, 99465, 102775, 117874, 102634, 104334, 111391, 100038, 99665, 17447, 114443, 111770, 106795, 16530, 102634, 29767, 102003, 101940, 99555, 101229, 103931, 108471, 103347, 101364, 99796, 107510, 101097, 99213, 77540, 14224, 80443, 104108, 81800, 80443, 115688, 101229, 109692, 99405, 104197, 101929, 100131, 106099, 91680, 99992, 100749, 100399, 46306, 80158, 101043, 52510, 34187, 100626, 112292, 104000, 99470, 47534, 102125, 100081, 100475, 27442, 9370, 103963, 99491, 105952, 100380, 16530, 105874, 112822, 104886, 99665, 44729, 103972, 99165, 100667, 105140, 100141, 99278, 20929, 14777, 9370, 71416, 101103, 100137, 47874, 109984, 55286, 80443, 101958, 100371, 45181, 104722, 100766, 100688, 104150, 107661, 45181, 100106, 99958, 100896, 113715, 30440, 101041, 109239, 151645, 198, 71703, 25, 106557, 151645, 198]\n",
            "inputs:\n",
            "Human: 你是一个经过训练的人工智能，可以分析文本输入，并根据上下文将其分类为最合适的情感类型。你的任务是仔细评估输入并确定输入属于哪种情绪。从以下情绪中选择：1. “正面”，2.“负面”。只提供情绪的名称作为输出，不提供额外的解释。\n",
            "第一次写食评赶上狂欢夏威夷主题自助8折人均540自助餐里算高价位而性价比非常高海鲜很新鲜而且供应不停龙虾钳剪好了摆在冰盘上也是很良心明明不剪开就可以减少很多供应除了煎鹅肝一次排队只能领两件没有限制数量没有削减供应海鲜吃的大满足但是饮料只有一杯喝完就只有水了还有草莓朱古力喷泉甜点的制作非常用心一点不随意服务员清理盘子亦很及时到位一般带加一的店都有这种服务就好了开始没有找到地方从地面绕过去回来才知道从朗豪坊楼上可直接穿过<|im_end|>\n",
            "Assistant:正面<|im_end|>\n",
            "\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 106557, 151645, 198]\n",
            "labels:\n",
            "正面<|im_end|>\n",
            "\n",
            "[INFO|configuration_utils.py:699] 2025-04-14 08:40:11,627 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-04-14 08:40:11,628 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3584,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 18944,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 28,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 4,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 131072,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.50.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 152064\n",
            "}\n",
            "\n",
            "[INFO|2025-04-14 08:40:11] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n",
            "[INFO|modeling_utils.py:1154] 2025-04-14 08:40:11,687 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:2170] 2025-04-14 08:40:11,688 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1139] 2025-04-14 08:40:11,690 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "[WARNING|logging.py:329] 2025-04-14 08:40:11,692 >> Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
            "Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.14s/it]\n",
            "[INFO|modeling_utils.py:4987] 2025-04-14 08:40:16,308 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4995] 2025-04-14 08:40:16,308 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at lordjia/Qwen2-Cantonese-7B-Instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:1094] 2025-04-14 08:40:16,365 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/generation_config.json\n",
            "[INFO|configuration_utils.py:1139] 2025-04-14 08:40:16,365 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    151645,\n",
            "    151643\n",
            "  ],\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"repetition_penalty\": 1.05,\n",
            "  \"temperature\": 0.7,\n",
            "  \"top_k\": 20,\n",
            "  \"top_p\": 0.8\n",
            "}\n",
            "\n",
            "[INFO|2025-04-14 08:40:16] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
            "[INFO|2025-04-14 08:40:16] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-04-14 08:40:16] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
            "[INFO|2025-04-14 08:40:16] llamafactory.model.adapter:143 >> Fine-tuning method: DoRA\n",
            "[INFO|2025-04-14 08:40:16] llamafactory.model.model_utils.misc:143 >> Found linear modules: up_proj,o_proj,v_proj,q_proj,down_proj,k_proj,gate_proj\n",
            "[INFO|2025-04-14 08:40:17] llamafactory.model.loader:143 >> trainable params: 21,575,680 || all params: 7,637,192,192 || trainable%: 0.2825\n",
            "[INFO|trainer.py:748] 2025-04-14 08:40:17,214 >> Using auto half precision backend\n",
            "[WARNING|trainer.py:783] 2025-04-14 08:40:17,215 >> No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "[INFO|trainer.py:2409] 2025-04-14 08:40:17,673 >> ***** Running training *****\n",
            "[INFO|trainer.py:2410] 2025-04-14 08:40:17,673 >>   Num examples = 11,289\n",
            "[INFO|trainer.py:2411] 2025-04-14 08:40:17,673 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2412] 2025-04-14 08:40:17,673 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:2415] 2025-04-14 08:40:17,673 >>   Total train batch size (w. parallel, distributed & accumulation) = 256\n",
            "[INFO|trainer.py:2416] 2025-04-14 08:40:17,673 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:2417] 2025-04-14 08:40:17,673 >>   Total optimization steps = 132\n",
            "[INFO|trainer.py:2418] 2025-04-14 08:40:17,677 >>   Number of trainable parameters = 21,575,680\n",
            "  4% 5/132 [03:00<1:16:25, 36.11s/it][INFO|2025-04-14 08:43:18] llamafactory.train.callbacks:143 >> {'loss': 1.9302, 'learning_rate': 4.9823e-05, 'epoch': 0.11, 'throughput': 1813.12}\n",
            "{'loss': 1.9302, 'grad_norm': 2.8312206268310547, 'learning_rate': 4.982319711683221e-05, 'epoch': 0.11, 'num_input_tokens_seen': 327680}\n",
            "  8% 10/132 [06:01<1:13:22, 36.08s/it][INFO|2025-04-14 08:46:18] llamafactory.train.callbacks:143 >> {'loss': 0.6856, 'learning_rate': 4.9295e-05, 'epoch': 0.23, 'throughput': 1814.85}\n",
            "{'loss': 0.6856, 'grad_norm': 2.541280508041382, 'learning_rate': 4.929528920808854e-05, 'epoch': 0.23, 'num_input_tokens_seen': 655360}\n",
            " 11% 15/132 [09:01<1:10:21, 36.08s/it][INFO|2025-04-14 08:49:19] llamafactory.train.callbacks:143 >> {'loss': 0.4585, 'learning_rate': 4.8424e-05, 'epoch': 0.34, 'throughput': 1815.33}\n",
            "{'loss': 0.4585, 'grad_norm': 3.113072633743286, 'learning_rate': 4.842374312499405e-05, 'epoch': 0.34, 'num_input_tokens_seen': 983040}\n",
            " 15% 20/132 [12:01<1:07:21, 36.08s/it][INFO|2025-04-14 08:52:19] llamafactory.train.callbacks:143 >> {'loss': 0.2192, 'learning_rate': 4.7221e-05, 'epoch': 0.45, 'throughput': 1815.61}\n",
            "{'loss': 0.2192, 'grad_norm': 3.249933958053589, 'learning_rate': 4.722088621637309e-05, 'epoch': 0.45, 'num_input_tokens_seen': 1310720}\n",
            " 19% 25/132 [15:02<1:04:20, 36.08s/it][INFO|2025-04-14 08:55:20] llamafactory.train.callbacks:143 >> {'loss': 0.1922, 'learning_rate': 4.5704e-05, 'epoch': 0.57, 'throughput': 1815.75}\n",
            "{'loss': 0.1922, 'grad_norm': 3.368380308151245, 'learning_rate': 4.570373196778427e-05, 'epoch': 0.57, 'num_input_tokens_seen': 1638400}\n",
            " 23% 30/132 [18:02<1:01:19, 36.08s/it][INFO|2025-04-14 08:58:20] llamafactory.train.callbacks:143 >> {'loss': 0.1391, 'learning_rate': 4.3894e-05, 'epoch': 0.68, 'throughput': 1815.88}\n",
            "{'loss': 0.1391, 'grad_norm': 3.196946382522583, 'learning_rate': 4.389373935885646e-05, 'epoch': 0.68, 'num_input_tokens_seen': 1966080}\n",
            " 27% 35/132 [21:03<58:19, 36.07s/it][INFO|2025-04-14 09:01:20] llamafactory.train.callbacks:143 >> {'loss': 0.1134, 'learning_rate': 4.1817e-05, 'epoch': 0.79, 'throughput': 1816.01}\n",
            "{'loss': 0.1134, 'grad_norm': 1.757630705833435, 'learning_rate': 4.181650934253132e-05, 'epoch': 0.79, 'num_input_tokens_seen': 2293760}\n",
            " 30% 40/132 [24:03<55:19, 36.08s/it][INFO|2025-04-14 09:04:21] llamafactory.train.callbacks:143 >> {'loss': 0.1020, 'learning_rate': 3.9501e-05, 'epoch': 0.91, 'throughput': 1816.06}\n",
            "{'loss': 0.102, 'grad_norm': 1.258773684501648, 'learning_rate': 3.9501422739279956e-05, 'epoch': 0.91, 'num_input_tokens_seen': 2621440}\n",
            " 34% 45/132 [26:31<38:13, 26.36s/it][INFO|2025-04-14 09:06:49] llamafactory.train.callbacks:143 >> {'loss': 0.1016, 'learning_rate': 3.6981e-05, 'epoch': 1.00, 'throughput': 1815.92}\n",
            "{'loss': 0.1016, 'grad_norm': 7.304866790771484, 'learning_rate': 3.6981224668001424e-05, 'epoch': 1.0, 'num_input_tokens_seen': 2889984}\n",
            " 38% 50/132 [29:31<47:04, 34.44s/it][INFO|2025-04-14 09:09:49] llamafactory.train.callbacks:143 >> {'loss': 0.0860, 'learning_rate': 3.4292e-05, 'epoch': 1.11, 'throughput': 1815.98}\n",
            "{'loss': 0.086, 'grad_norm': 0.3709675967693329, 'learning_rate': 3.4291561391508185e-05, 'epoch': 1.11, 'num_input_tokens_seen': 3217664}\n",
            " 42% 55/132 [32:32<45:56, 35.80s/it][INFO|2025-04-14 09:12:49] llamafactory.train.callbacks:143 >> {'loss': 0.0916, 'learning_rate': 3.1470e-05, 'epoch': 1.23, 'throughput': 1816.04}\n",
            "{'loss': 0.0916, 'grad_norm': 1.04539954662323, 'learning_rate': 3.147047612756302e-05, 'epoch': 1.23, 'num_input_tokens_seen': 3545344}\n",
            " 45% 60/132 [35:32<43:14, 36.03s/it][INFO|2025-04-14 09:15:50] llamafactory.train.callbacks:143 >> {'loss': 0.0894, 'learning_rate': 2.8558e-05, 'epoch': 1.34, 'throughput': 1816.08}\n",
            "{'loss': 0.0894, 'grad_norm': 0.5506842732429504, 'learning_rate': 2.8557870956832132e-05, 'epoch': 1.34, 'num_input_tokens_seen': 3873024}\n",
            " 49% 65/132 [38:32<40:16, 36.06s/it][INFO|2025-04-14 09:18:50] llamafactory.train.callbacks:143 >> {'loss': 0.0815, 'learning_rate': 2.5595e-05, 'epoch': 1.45, 'throughput': 1816.14}\n",
            "{'loss': 0.0815, 'grad_norm': 0.4645310938358307, 'learning_rate': 2.5594942438652688e-05, 'epoch': 1.45, 'num_input_tokens_seen': 4200704}\n",
            " 53% 70/132 [41:33<37:16, 36.06s/it][INFO|2025-04-14 09:21:50] llamafactory.train.callbacks:143 >> {'loss': 0.0762, 'learning_rate': 2.2624e-05, 'epoch': 1.57, 'throughput': 1816.13}\n",
            "{'loss': 0.0762, 'grad_norm': 0.8913150429725647, 'learning_rate': 2.2623598917395438e-05, 'epoch': 1.57, 'num_input_tokens_seen': 4528128}\n",
            " 57% 75/132 [44:33<34:16, 36.08s/it][INFO|2025-04-14 09:24:51] llamafactory.train.callbacks:143 >> {'loss': 0.0839, 'learning_rate': 1.9686e-05, 'epoch': 1.68, 'throughput': 1816.13}\n",
            "{'loss': 0.0839, 'grad_norm': 0.7342696189880371, 'learning_rate': 1.9685867761175584e-05, 'epoch': 1.68, 'num_input_tokens_seen': 4855808}\n",
            " 61% 80/132 [47:34<31:16, 36.08s/it][INFO|2025-04-14 09:27:51] llamafactory.train.callbacks:143 >> {'loss': 0.0871, 'learning_rate': 1.6823e-05, 'epoch': 1.79, 'throughput': 1816.14}\n",
            "{'loss': 0.0871, 'grad_norm': 0.898593008518219, 'learning_rate': 1.682330091706446e-05, 'epoch': 1.79, 'num_input_tokens_seen': 5183488}\n",
            " 64% 85/132 [50:34<28:15, 36.08s/it][INFO|2025-04-14 09:30:52] llamafactory.train.callbacks:143 >> {'loss': 0.0810, 'learning_rate': 1.4076e-05, 'epoch': 1.91, 'throughput': 1816.18}\n",
            "{'loss': 0.081, 'grad_norm': 0.4044731557369232, 'learning_rate': 1.4076387190766017e-05, 'epoch': 1.91, 'num_input_tokens_seen': 5511168}\n",
            " 68% 90/132 [53:02<18:26, 26.36s/it][INFO|2025-04-14 09:33:20] llamafactory.train.callbacks:143 >> {'loss': 0.0742, 'learning_rate': 1.1484e-05, 'epoch': 2.00, 'throughput': 1816.10}\n",
            "{'loss': 0.0742, 'grad_norm': 0.8598770499229431, 'learning_rate': 1.148397956361007e-05, 'epoch': 2.0, 'num_input_tokens_seen': 5779712}\n",
            " 72% 95/132 [56:02<21:14, 34.44s/it][INFO|2025-04-14 09:36:20] llamafactory.train.callbacks:143 >> {'loss': 0.0753, 'learning_rate': 9.0827e-06, 'epoch': 2.11, 'throughput': 1816.12}\n",
            "{'loss': 0.0753, 'grad_norm': 0.8975450396537781, 'learning_rate': 9.082745647022797e-06, 'epoch': 2.11, 'num_input_tokens_seen': 6107392}\n",
            " 76% 100/132 [59:03<19:05, 35.80s/it][INFO|2025-04-14 09:39:20] llamafactory.train.callbacks:143 >> {'loss': 0.0743, 'learning_rate': 6.9066e-06, 'epoch': 2.23, 'throughput': 1816.15}\n",
            "{'loss': 0.0743, 'grad_norm': 0.7114613652229309, 'learning_rate': 6.906649047373246e-06, 'epoch': 2.23, 'num_input_tokens_seen': 6435072}\n",
            " 76% 100/132 [59:03<19:05, 35.80s/it][INFO|trainer.py:4289] 2025-04-14 09:39:20,928 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4291] 2025-04-14 09:39:20,928 >>   Num examples = 595\n",
            "[INFO|trainer.py:4294] 2025-04-14 09:39:20,928 >>   Batch size = 32\n",
            "\n",
            "  0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            " 11% 2/19 [00:01<00:12,  1.35it/s]\u001b[A\n",
            " 16% 3/19 [00:02<00:16,  1.05s/it]\u001b[A\n",
            " 21% 4/19 [00:04<00:18,  1.21s/it]\u001b[A\n",
            " 26% 5/19 [00:05<00:18,  1.30s/it]\u001b[A\n",
            " 32% 6/19 [00:07<00:17,  1.36s/it]\u001b[A\n",
            " 37% 7/19 [00:08<00:16,  1.40s/it]\u001b[A\n",
            " 42% 8/19 [00:10<00:15,  1.42s/it]\u001b[A\n",
            " 47% 9/19 [00:11<00:14,  1.44s/it]\u001b[A\n",
            " 53% 10/19 [00:13<00:13,  1.45s/it]\u001b[A\n",
            " 58% 11/19 [00:14<00:11,  1.46s/it]\u001b[A\n",
            " 63% 12/19 [00:16<00:10,  1.47s/it]\u001b[A\n",
            " 68% 13/19 [00:17<00:08,  1.47s/it]\u001b[A\n",
            " 74% 14/19 [00:19<00:07,  1.47s/it]\u001b[A\n",
            " 79% 15/19 [00:20<00:05,  1.47s/it]\u001b[A\n",
            " 84% 16/19 [00:22<00:04,  1.48s/it]\u001b[A\n",
            " 89% 17/19 [00:23<00:02,  1.48s/it]\u001b[A\n",
            " 95% 18/19 [00:25<00:01,  1.47s/it]\u001b[A\n",
            "                                     \n",
            "\u001b[A{'eval_loss': 0.09139470756053925, 'eval_runtime': 27.6021, 'eval_samples_per_second': 21.556, 'eval_steps_per_second': 0.688, 'epoch': 2.23, 'num_input_tokens_seen': 6435072}\n",
            " 76% 100/132 [59:30<19:05, 35.80s/it]\n",
            "100% 19/19 [00:26<00:00,  1.33s/it]\u001b[A\n",
            "                                   \u001b[A[INFO|trainer.py:3966] 2025-04-14 09:39:48,531 >> Saving model checkpoint to saves/Custom/lora/train_2025-04-14-08-39-09/checkpoint-100\n",
            "[INFO|configuration_utils.py:699] 2025-04-14 09:39:48,842 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-04-14 09:39:48,842 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3584,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 18944,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 28,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 4,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 131072,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.50.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 152064\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-14 09:39:49,046 >> tokenizer config file saved in saves/Custom/lora/train_2025-04-14-08-39-09/checkpoint-100/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-14 09:39:49,047 >> Special tokens file saved in saves/Custom/lora/train_2025-04-14-08-39-09/checkpoint-100/special_tokens_map.json\n",
            " 80% 105/132 [1:02:32<17:08, 38.10s/it][INFO|2025-04-14 09:42:49] llamafactory.train.callbacks:143 >> {'loss': 0.0715, 'learning_rate': 4.9865e-06, 'epoch': 2.34, 'throughput': 1802.30}\n",
            "{'loss': 0.0715, 'grad_norm': 0.384044349193573, 'learning_rate': 4.986468976890993e-06, 'epoch': 2.34, 'num_input_tokens_seen': 6762752}\n",
            " 83% 110/132 [1:05:32<13:21, 36.42s/it][INFO|2025-04-14 09:45:50] llamafactory.train.callbacks:143 >> {'loss': 0.0736, 'learning_rate': 3.3494e-06, 'epoch': 2.45, 'throughput': 1802.95}\n",
            "{'loss': 0.0736, 'grad_norm': 0.34839120507240295, 'learning_rate': 3.3493649053890326e-06, 'epoch': 2.45, 'num_input_tokens_seen': 7090432}\n",
            " 87% 115/132 [1:08:33<10:14, 36.14s/it][INFO|2025-04-14 09:48:50] llamafactory.train.callbacks:143 >> {'loss': 0.0757, 'learning_rate': 2.0185e-06, 'epoch': 2.57, 'throughput': 1803.53}\n",
            "{'loss': 0.0757, 'grad_norm': 0.6813746690750122, 'learning_rate': 2.0184924104583613e-06, 'epoch': 2.57, 'num_input_tokens_seen': 7418112}\n",
            " 91% 120/132 [1:11:33<07:13, 36.09s/it][INFO|2025-04-14 09:51:51] llamafactory.train.callbacks:143 >> {'loss': 0.0778, 'learning_rate': 1.0127e-06, 'epoch': 2.68, 'throughput': 1804.07}\n",
            "{'loss': 0.0778, 'grad_norm': 0.3634191155433655, 'learning_rate': 1.0126756596375686e-06, 'epoch': 2.68, 'num_input_tokens_seen': 7745792}\n",
            " 95% 125/132 [1:14:33<04:12, 36.08s/it][INFO|2025-04-14 09:54:51] llamafactory.train.callbacks:143 >> {'loss': 0.0728, 'learning_rate': 3.4614e-07, 'epoch': 2.79, 'throughput': 1804.59}\n",
            "{'loss': 0.0728, 'grad_norm': 0.6371954679489136, 'learning_rate': 3.4614115704533767e-07, 'epoch': 2.79, 'num_input_tokens_seen': 8073472}\n",
            " 98% 130/132 [1:17:34<01:12, 36.07s/it][INFO|2025-04-14 09:57:51] llamafactory.train.callbacks:143 >> {'loss': 0.0758, 'learning_rate': 2.8317e-08, 'epoch': 2.91, 'throughput': 1805.06}\n",
            "{'loss': 0.0758, 'grad_norm': 0.31691211462020874, 'learning_rate': 2.831652042480093e-08, 'epoch': 2.91, 'num_input_tokens_seen': 8401152}\n",
            "100% 132/132 [1:18:46<00:00, 36.07s/it][INFO|trainer.py:3966] 2025-04-14 09:59:04,045 >> Saving model checkpoint to saves/Custom/lora/train_2025-04-14-08-39-09/checkpoint-132\n",
            "[INFO|configuration_utils.py:699] 2025-04-14 09:59:04,191 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-04-14 09:59:04,192 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3584,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 18944,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 28,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 4,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 131072,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.50.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 152064\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-14 09:59:04,356 >> tokenizer config file saved in saves/Custom/lora/train_2025-04-14-08-39-09/checkpoint-132/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-14 09:59:04,356 >> Special tokens file saved in saves/Custom/lora/train_2025-04-14-08-39-09/checkpoint-132/special_tokens_map.json\n",
            "[INFO|trainer.py:2665] 2025-04-14 09:59:04,910 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 4727.2322, 'train_samples_per_second': 7.164, 'train_steps_per_second': 0.028, 'train_loss': 0.20131369894652656, 'epoch': 2.95, 'num_input_tokens_seen': 8532224}\n",
            "100% 132/132 [1:18:47<00:00, 35.81s/it]\n",
            "[INFO|trainer.py:3966] 2025-04-14 09:59:04,911 >> Saving model checkpoint to saves/Custom/lora/train_2025-04-14-08-39-09\n",
            "[INFO|configuration_utils.py:699] 2025-04-14 09:59:05,017 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-04-14 09:59:05,018 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3584,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 18944,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 28,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 4,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 131072,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.50.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 152064\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2510] 2025-04-14 09:59:05,177 >> tokenizer config file saved in saves/Custom/lora/train_2025-04-14-08-39-09/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2519] 2025-04-14 09:59:05,177 >> Special tokens file saved in saves/Custom/lora/train_2025-04-14-08-39-09/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =      2.9518\n",
            "  num_input_tokens_seen    =     8532224\n",
            "  total_flos               = 338138238GF\n",
            "  train_loss               =      0.2013\n",
            "  train_runtime            =  1:18:47.23\n",
            "  train_samples_per_second =       7.164\n",
            "  train_steps_per_second   =       0.028\n",
            "Figure saved at: saves/Custom/lora/train_2025-04-14-08-39-09/training_loss.png\n",
            "Figure saved at: saves/Custom/lora/train_2025-04-14-08-39-09/training_eval_loss.png\n",
            "[WARNING|2025-04-14 09:59:05] llamafactory.extras.ploting:148 >> No metric eval_accuracy to plot.\n",
            "[INFO|trainer.py:4289] 2025-04-14 09:59:05,688 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4291] 2025-04-14 09:59:05,689 >>   Num examples = 595\n",
            "[INFO|trainer.py:4294] 2025-04-14 09:59:05,689 >>   Batch size = 32\n",
            "100% 19/19 [00:26<00:00,  1.37s/it]\n",
            "***** eval metrics *****\n",
            "  epoch                   =     2.9518\n",
            "  eval_loss               =     0.0888\n",
            "  eval_runtime            = 0:00:27.61\n",
            "  eval_samples_per_second =     21.543\n",
            "  eval_steps_per_second   =      0.688\n",
            "  num_input_tokens_seen   =    8532224\n",
            "[INFO|modelcard.py:449] 2025-04-14 09:59:33,309 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
            "2025-04-14 10:19:01.881930: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1744625941.903835   51764 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1744625941.910499   51764 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "[INFO|2025-04-14 10:19:08] llamafactory.hparams.parser:379 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: None\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 10:19:08,506 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 10:19:08,507 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 10:19:08,507 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 10:19:08,507 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 10:19:08,507 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 10:19:08,507 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 10:19:08,507 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2323] 2025-04-14 10:19:08,882 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|configuration_utils.py:699] 2025-04-14 10:19:09,094 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-04-14 10:19:09,096 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3584,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 18944,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 28,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 4,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 131072,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.50.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 152064\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 10:19:09,145 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 10:19:09,145 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 10:19:09,145 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 10:19:09,145 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 10:19:09,145 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 10:19:09,145 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2060] 2025-04-14 10:19:09,145 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|tokenization_utils_base.py:2323] 2025-04-14 10:19:09,500 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "[INFO|2025-04-14 10:19:09] llamafactory.data.loader:143 >> Loading dataset Openrice_test_alpaca_with_emojis_texts.json...\n",
            "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
            "Generating train split: 6128 examples [00:00, 75764.40 examples/s]\n",
            "Converting format of dataset (num_proc=16): 100% 6128/6128 [00:00<00:00, 19633.34 examples/s]\n",
            "Running tokenizer on dataset (num_proc=16): 100% 6128/6128 [00:03<00:00, 1836.60 examples/s]\n",
            "eval example:\n",
            "input_ids:\n",
            "[33975, 25, 220, 56568, 101909, 101897, 104034, 100623, 48692, 100168, 3837, 73670, 101042, 108704, 31196, 90395, 100345, 102285, 16744, 105278, 70538, 17714, 117585, 109955, 31905, 1773, 103929, 88802, 20412, 104857, 102086, 31196, 62926, 60610, 31196, 100409, 113195, 104405, 1773, 45181, 87752, 104405, 15946, 50404, 5122, 16, 13, 1036, 106557, 33590, 17, 34047, 103276, 55807, 91680, 99553, 104405, 9370, 29991, 100622, 66017, 3837, 16530, 99553, 108593, 9370, 104136, 8997, 102242, 101443, 114016, 55338, 118952, 103102, 108703, 26939, 44, 60204, 99416, 102171, 99416, 102171, 24562, 11, 30534, 99738, 105925, 115009, 60726, 102810, 99593, 99542, 99765, 42468, 100294, 99512, 11, 103532, 101570, 104091, 52801, 99375, 99662, 101655, 11, 77288, 99765, 42468, 100294, 69442, 32757, 101461, 82647, 104686, 11, 63431, 100480, 116433, 30709, 34187, 11, 99271, 52853, 120110, 108871, 105793, 104334, 11, 106053, 105925, 20412, 99369, 119101, 11, 77288, 104302, 105161, 99416, 102171, 34187, 151645, 198, 71703, 25]\n",
            "inputs:\n",
            "Human: 你是一个经过训练的人工智能，可以分析文本输入，并根据上下文将其分类为最合适的情感类型。你的任务是仔细评估输入并确定输入属于哪种情绪。从以下情绪中选择：1. “正面”，2.“负面”。只提供情绪的名称作为输出，不提供额外的解释。\n",
            "今日帶齊所有考試範圍到M記温書温書前,要令肚子乖乖先買了一個巨無霸餐,薯條依然好味香脆,但巨無霸份量則少了很多,整體縮小了,百物昴貴嗎好了,雖然肚子是半飽,但也要開始温書了<|im_end|>\n",
            "Assistant:\n",
            "label_ids:\n",
            "[106557, 151645, 198]\n",
            "labels:\n",
            "正面<|im_end|>\n",
            "\n",
            "[INFO|configuration_utils.py:699] 2025-04-14 10:19:14,327 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/config.json\n",
            "[INFO|configuration_utils.py:771] 2025-04-14 10:19:14,328 >> Model config Qwen2Config {\n",
            "  \"architectures\": [\n",
            "    \"Qwen2ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3584,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 18944,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"max_window_layers\": 28,\n",
            "  \"model_type\": \"qwen2\",\n",
            "  \"num_attention_heads\": 28,\n",
            "  \"num_hidden_layers\": 28,\n",
            "  \"num_key_value_heads\": 4,\n",
            "  \"rms_norm_eps\": 1e-06,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 1000000.0,\n",
            "  \"sliding_window\": 131072,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.50.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"use_sliding_window\": false,\n",
            "  \"vocab_size\": 152064\n",
            "}\n",
            "\n",
            "[INFO|2025-04-14 10:19:14] llamafactory.model.model_utils.kv_cache:143 >> KV cache is enabled for faster generation.\n",
            "[INFO|modeling_utils.py:1154] 2025-04-14 10:19:14,395 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/model.safetensors.index.json\n",
            "[INFO|modeling_utils.py:2170] 2025-04-14 10:19:14,396 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1139] 2025-04-14 10:19:14,398 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"eos_token_id\": 151645\n",
            "}\n",
            "\n",
            "[WARNING|logging.py:329] 2025-04-14 10:19:14,400 >> Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
            "Loading checkpoint shards: 100% 4/4 [00:04<00:00,  1.13s/it]\n",
            "[INFO|modeling_utils.py:4987] 2025-04-14 10:19:18,980 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4995] 2025-04-14 10:19:18,980 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at lordjia/Qwen2-Cantonese-7B-Instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:1094] 2025-04-14 10:19:19,031 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--lordjia--Qwen2-Cantonese-7B-Instruct/snapshots/0fadd5e158c2fc6f0f605a79d5d57c6e68ff4e20/generation_config.json\n",
            "[INFO|configuration_utils.py:1139] 2025-04-14 10:19:19,031 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 151643,\n",
            "  \"do_sample\": true,\n",
            "  \"eos_token_id\": [\n",
            "    151645,\n",
            "    151643\n",
            "  ],\n",
            "  \"pad_token_id\": 151643,\n",
            "  \"repetition_penalty\": 1.05,\n",
            "  \"temperature\": 0.7,\n",
            "  \"top_k\": 20,\n",
            "  \"top_p\": 0.8\n",
            "}\n",
            "\n",
            "[INFO|2025-04-14 10:19:19] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-04-14 10:19:20] llamafactory.model.adapter:143 >> Merged 1 adapter(s).\n",
            "[INFO|2025-04-14 10:19:20] llamafactory.model.adapter:143 >> Loaded adapter(s): saves/Custom/lora/train_2025-04-14-08-39-09\n",
            "[INFO|2025-04-14 10:19:20] llamafactory.model.loader:143 >> all params: 7,615,616,512\n",
            "[WARNING|2025-04-14 10:19:20] llamafactory.train.sft.workflow:154 >> Batch generation can be very slow. Consider using `scripts/vllm_infer.py` instead.\n",
            "[INFO|trainer.py:4289] 2025-04-14 10:19:20,345 >> \n",
            "***** Running Prediction *****\n",
            "[INFO|trainer.py:4291] 2025-04-14 10:19:20,345 >>   Num examples = 6128\n",
            "[INFO|trainer.py:4294] 2025-04-14 10:19:20,345 >>   Batch size = 32\n",
            "100% 192/192 [02:01<00:00,  1.81it/s]Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /tmp/jieba.cache\n",
            "Loading model cost 0.800 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "[INFO|integration_utils.py:831] 2025-04-14 10:21:25,345 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcharlotte-geng111\u001b[0m (\u001b[33mcharlotte-geng111-wenzhou-kean-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/LLaMA-Factory/wandb/run-20250414_102125-yecvila3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msaves/Custom/lora/eval_2025-04-14-08-39-09\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/charlotte-geng111-wenzhou-kean-university/llamafactory\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/charlotte-geng111-wenzhou-kean-university/llamafactory/runs/yecvila3\u001b[0m\n",
            "100% 192/192 [02:04<00:00,  1.54it/s]\n",
            "***** predict metrics *****\n",
            "  predict_bleu-4                 =    34.2725\n",
            "  predict_model_preparation_time =     0.0047\n",
            "  predict_rouge-1                =     90.062\n",
            "  predict_rouge-2                =        0.0\n",
            "  predict_rouge-l                =     90.062\n",
            "  predict_runtime                = 0:02:05.00\n",
            "  predict_samples_per_second     =     49.024\n",
            "  predict_steps_per_second       =      1.536\n",
            "[INFO|2025-04-14 10:21:26] llamafactory.train.sft.trainer:143 >> Saving prediction results to saves/Custom/lora/eval_2025-04-14-08-39-09/generated_predictions.jsonl\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33msaves/Custom/lora/eval_2025-04-14-08-39-09\u001b[0m at: \u001b[34mhttps://wandb.ai/charlotte-geng111-wenzhou-kean-university/llamafactory/runs/yecvila3\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250414_102125-yecvila3/logs\u001b[0m\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2963, in block_thread\n",
            "    time.sleep(0.1)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/llamafactory-cli\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/cli.py\", line 121, in main\n",
            "    run_web_ui()\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/webui/interface.py\", line 97, in run_web_ui\n",
            "    create_ui().queue().launch(share=gradio_share, server_name=server_name, inbrowser=True)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2869, in launch\n",
            "    self.block_thread()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 2965, in block_thread\n",
            "    print(\"Keyboard interruption in main thread... closing server.\")\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/wandb/sdk/lib/console_capture.py\", line 147, in write_with_callbacks\n",
            "    n = orig_write(s)\n",
            "        ^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "Killing tunnel 0.0.0.0:7860 <> https://37a019eefe6495ca24.gradio.live\n"
          ]
        }
      ],
      "source": [
        "%cd /content/LLaMA-Factory/\n",
        "!GRADIO_SHARE=1 llamafactory-cli webui"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4e69c10ea9b34a54b7bbffb1fe360a7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_1ef7bcac08ba444ebf2d980391a36e0c"
          }
        },
        "2fe96ee737c64d9b8011018d04205d19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f12f7ba7f49416abc1b73005ee1d616",
            "placeholder": "​",
            "style": "IPY_MODEL_8d96e2391bc94f94b83707048de46730",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "09708cceb7244359961cc36ef0dc0ccb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_06a9d42bc1f84c1488f3fe9f7191588c",
            "placeholder": "​",
            "style": "IPY_MODEL_e09c15b7dd2a4a7793b15680e0621b18",
            "value": ""
          }
        },
        "fb776a80517d4fa19ecc5ecb8d49ed8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_2be2ffc36d794580a02b702c34d93471",
            "style": "IPY_MODEL_a60023e5979645b3ae90e8eeaa1ef595",
            "value": true
          }
        },
        "b0b8270eddb24107b2c511fa105b8dc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_91c9269f80fc400a8449057b07345c16",
            "style": "IPY_MODEL_5cd71d1da66f45e3adddfc6811929b31",
            "tooltip": ""
          }
        },
        "aae756c717ea41b09f16da68fd2d5481": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5dbb89ed0a114202be3742f4bf0efdb5",
            "placeholder": "​",
            "style": "IPY_MODEL_38b4972dc534429e9e3a4aa6a244b715",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "1ef7bcac08ba444ebf2d980391a36e0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "3f12f7ba7f49416abc1b73005ee1d616": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d96e2391bc94f94b83707048de46730": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "06a9d42bc1f84c1488f3fe9f7191588c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e09c15b7dd2a4a7793b15680e0621b18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2be2ffc36d794580a02b702c34d93471": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a60023e5979645b3ae90e8eeaa1ef595": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91c9269f80fc400a8449057b07345c16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cd71d1da66f45e3adddfc6811929b31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "5dbb89ed0a114202be3742f4bf0efdb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38b4972dc534429e9e3a4aa6a244b715": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e55899ce73c14a3ab3052c2b406a8f88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ebdd055fdfc447faef1c1ff26c20125",
            "placeholder": "​",
            "style": "IPY_MODEL_0d742f3d3a9f4423a77b465338550ca1",
            "value": "Connecting..."
          }
        },
        "7ebdd055fdfc447faef1c1ff26c20125": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d742f3d3a9f4423a77b465338550ca1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}