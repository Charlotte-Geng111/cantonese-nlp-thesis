{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1520b300",
   "metadata": {},
   "source": [
    "### 1. Conver .txt file to .json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72bd8eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è½¬æ¢å®Œæˆï¼Œæ•°æ®é›†å·²ä¿å­˜ä¸º JSON æ–‡ä»¶ï¼šOpenrice_Cantonese.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "input_file = \"Openrice_Cantonese.txt\"  \n",
    "output_file = \"Openrice_Cantonese.json\"  \n",
    "\n",
    "formatted_data = []\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()  \n",
    "        if not line:  \n",
    "            continue\n",
    "        try:\n",
    "            # Separate CLASS and TEXT using \\t\\t\n",
    "            rating, review = line.split(\"\\t\\t\", 1)  \n",
    "            formatted_data.append({\"text\": review, \"label\": int(rating)})  \n",
    "        except ValueError:\n",
    "            print(f\"è·³è¿‡æ ¼å¼é”™è¯¯çš„è¡Œ: {line}\") #\"Misformed lines are skipped:\"\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(formatted_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"è½¬æ¢å®Œæˆï¼Œæ•°æ®é›†å·²ä¿å­˜ä¸º JSON æ–‡ä»¶ï¼š{output_file}\") #\"The conversion is complete and the dataset has been saved as a JSON file:\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715a68ee",
   "metadata": {},
   "source": [
    "### 2. Divided into training set and testing set\n",
    "#### â€œAfter stochastic shuffling, 90% of dataset is used as the training set, while the other 10% reviews are used as the testing set.â€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c45276a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è®­ç»ƒé›†å·²ä¿å­˜ä¸ºï¼šOpenrice_train.json\n",
      "æµ‹è¯•é›†å·²ä¿å­˜ä¸ºï¼šOpenrice_test.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "input_file = \"Openrice_Cantonese.json\"  # è½¬æ¢åçš„ JSON æ–‡ä»¶è·¯å¾„\n",
    "train_file = \"Openrice_train.json\"  # è®­ç»ƒé›†æ–‡ä»¶è·¯å¾„\n",
    "test_file = \"Openrice_test.json\"  # æµ‹è¯•é›†æ–‡ä»¶è·¯å¾„\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# shuffle data randomly\n",
    "random.shuffle(data)\n",
    "\n",
    "# Split training and test set at 90% and 10%\n",
    "train_size = int(0.9 * len(data))  # 90% used as training set\n",
    "train_data = data[:train_size]\n",
    "test_data = data[train_size:]\n",
    "\n",
    "with open(train_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(train_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(test_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(test_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"è®­ç»ƒé›†å·²ä¿å­˜ä¸ºï¼š{train_file}\")\n",
    "print(f\"æµ‹è¯•é›†å·²ä¿å­˜ä¸ºï¼š{test_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bc3c5c",
   "metadata": {},
   "source": [
    "### 3. Before Dataset preprocessing, the label is changed to have positive and negative emotions (1,2 is negative; 3,4,5 are heads)\n",
    "### 3. åœ¨Dataset preprocessingå‰ï¼Œå…ˆå°†labelæ”¹ä¸ºæ­£é¢æƒ…ç»ªå’Œè´Ÿé¢æƒ…ç»ªä¸¤ç§ï¼ˆ1ï¼Œ2ä¸ºè´Ÿé¢ï¼›3ï¼Œ4ï¼Œ5ä¸ºæ­£é¢ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70a415fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ•°æ®å·²å¤„ç†å¹¶ä¿å­˜ä¸º Openrice_test_labeled.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def relabel_data(input_file, output_file):\n",
    "    # è¯»å– JSON æ•°æ®\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # é‡æ–°æ ‡æ³¨ label\n",
    "    for entry in data:\n",
    "        if entry[\"label\"] in [1, 2]:\n",
    "            entry[\"label\"] = \"è´Ÿé¢\"\n",
    "        elif entry[\"label\"] in [3, 4, 5]:\n",
    "            entry[\"label\"] = \"æ­£é¢\"\n",
    "\n",
    "    # ä¿å­˜ä¿®æ”¹åçš„æ•°æ®\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"æ•°æ®å·²å¤„ç†å¹¶ä¿å­˜ä¸º {output_file}\")\n",
    "\n",
    "# æ‰§è¡Œä»£ç \n",
    "input_file = \"Openrice_train.json\"\n",
    "output_file = \"Openrice_train_labeled.json\"\n",
    "input_file = \"Openrice_test.json\"\n",
    "output_file = \"Openrice_test_labeled.json\"\n",
    "\n",
    "relabel_data(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851add32",
   "metadata": {},
   "source": [
    "### 4. Adjust sample balance (training set only)\n",
    "#### â‘  First, count the number of positive and negative samples\n",
    "#### â‘¡ The number of additional positive samples is randomly removed, so that the number of positive samples and negative samples are the same\n",
    "#### Satisfies: the total number of samples is unchanged; Positive and negative sample equalization; Reduce positive samples (random sampling)\n",
    "\n",
    "### 4. è°ƒæ•´æ ·æœ¬å¹³è¡¡ (ä»…é’ˆå¯¹è®­ç»ƒé›†)\n",
    "#### â‘ å…ˆè®¡ç®—æ­£é¢å’Œè´Ÿé¢çš„æ ·æœ¬æ•°é‡\n",
    "#### â‘¡éšæœºå»é™¤å¤šå‡ºçš„æ­£é¢æ ·æœ¬æ•°é‡ï¼Œä»¤æ­£é¢æ ·æœ¬æ•°å’Œè´Ÿé¢æ ·æœ¬æ•°ç›¸åŒ\n",
    "#### æ»¡è¶³ï¼šæ€»æ ·æœ¬æ•°ä¸å˜ï¼› æ­£è´Ÿé¢æ ·æœ¬å‡è¡¡ï¼› å‡å°‘æ­£é¢æ ·æœ¬ï¼ˆéšæœºé‡‡æ ·ï¼‰ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7885dff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£é¢æ ·æœ¬æ•°é‡: 49485\n",
      "è´Ÿé¢æ ·æœ¬æ•°é‡: 5964\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def count_labels(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Count positive and negative samples\n",
    "    positive_count = sum(1 for entry in data if entry[\"label\"] == \"æ­£é¢\")\n",
    "    negative_count = sum(1 for entry in data if entry[\"label\"] == \"è´Ÿé¢\")\n",
    "\n",
    "    print(f\"æ­£é¢æ ·æœ¬æ•°é‡: {positive_count}\")\n",
    "    print(f\"è´Ÿé¢æ ·æœ¬æ•°é‡: {negative_count}\")\n",
    "\n",
    "file_path = \"Openrice_train_labeled.json\"\n",
    "# file_path = \"Openrice_train_balanced2.json\"\n",
    "\n",
    "count_labels(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f94d2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š åŸå§‹æ•°æ®é›†: æ­£é¢ = 49485ï¼Œè´Ÿé¢ = 5964\n",
      "âœ… æ–°æ•°æ®é›†: 11928 æ¡æ•°æ®ï¼ˆæ­£è´Ÿå¹³è¡¡ï¼‰\n",
      "âœ… å¢å¼ºæ•°æ®å·²ä¿å­˜è‡³ Openrice_train_balanced2.jsonï¼\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "input_file = \"Openrice_train_labeled.json\"\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# distinguish between positive and negative examples\n",
    "positive_samples = [d for d in data if d[\"label\"] == \"æ­£é¢\"]  # positive\n",
    "negative_samples = [d for d in data if d[\"label\"] == \"è´Ÿé¢\"]  # negative\n",
    "\n",
    "print(f\"åŸå§‹æ•°æ®é›†: æ­£é¢ = {len(positive_samples)}ï¼Œè´Ÿé¢ = {len(negative_samples)}\")\n",
    "\n",
    "# Adjust the sample balance: reduce the number of positive samples (random samples)\n",
    "target_size = len(negative_samples) * 2\n",
    "positive_samples = random.sample(positive_samples, target_size - len(negative_samples))\n",
    "\n",
    "final_data = positive_samples + negative_samples\n",
    "\n",
    "print(f\"æ–°æ•°æ®é›†: {len(final_data)} æ¡æ•°æ®ï¼ˆæ­£è´Ÿå¹³è¡¡ï¼‰\")\n",
    "\n",
    "output_file = \"Openrice_train_balanced2.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(final_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"å¢å¼ºæ•°æ®å·²ä¿å­˜è‡³ {output_file}ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef8a577",
   "metadata": {},
   "source": [
    "### 5. Dataset preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6c7151",
   "metadata": {},
   "source": [
    "#### åˆ é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œå¹¶åˆ é™¤å¤šä½™çš„ç©ºæ ¼ã€‚\n",
    "#### åˆ é™¤éç²¤è¯­è¯„è®ºã€‚åœ¨æ•°æ®é›†ä¸­åŒ…å«éç²¤è¯­è¯„è®ºï¼Œä¾‹å¦‚è‹±è¯­æˆ–å…¶ä»–è¯­è¨€çš„è¯„è®ºï¼Œå¯èƒ½ä¼šå¹²æ‰°æ¨¡å‹çš„è®­ç»ƒã€‚\n",
    "#### å’Œâ€œè¯„è®ºçš„é•¿åº¦é™åˆ¶ä¸º250ä¸ªå­—ç¬¦â€\n",
    "#### \n",
    "#### Remove special characters, and remove excess spaces.\n",
    "#### Delete non-Cantonese comments. The inclusion of non-Cantonese comments in the dataset, such as in English or other languages, may interfere with the training of the model.\n",
    "#### And \"The length of comment is limited to 250 characters.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958a2c26",
   "metadata": {},
   "source": [
    "#### 1. no emojis å»é™¤è¡¨æƒ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2f02a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é¢„å¤„ç†å®Œæˆï¼Œæ•°æ®é›†å·²ä¿å­˜ä¸ºï¼šOpenrice_train_cleaned_no_emojis.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "# è¾“å…¥å’Œè¾“å‡ºæ–‡ä»¶è·¯å¾„\n",
    "input_train_file = \"Openrice_train_balanced2.json\"\n",
    "# input_test_file = \"Openrice_test_labeled.json\" \n",
    "\n",
    "output_train_file = \"Openrice_train_cleaned_no_emojis.json\"  # æ¸…ç†åçš„è®­ç»ƒé›†æ–‡ä»¶\n",
    "# output_test_file = \"Openrice_test_cleaned_no_emojis.json\"  # æ¸…ç†åçš„æµ‹è¯•é›†æ–‡ä»¶\n",
    "\n",
    "# å®šä¹‰æ¸…ç†å‡½æ•°\n",
    "def clean_text_no_emojis(text):\n",
    "    # ç§»é™¤ <sssss> æˆ–å…¶ä»–æ— æ•ˆå ä½ç¬¦\n",
    "    text = re.sub(r'<sssss>', '', text)\n",
    "    # ç§»é™¤ emoji å’Œæ— æ•ˆç‰¹æ®Šå­—ç¬¦\n",
    "    text = re.sub(r'[^\\w\\s.,!?]', '', text)\n",
    "    # å»é™¤å¤šä½™ç©ºæ ¼\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# å®šä¹‰è¯­è¨€æ£€æµ‹å‡½æ•°ï¼ˆè¿‡æ»¤éç²¤è¯­è¯„è®ºï¼‰\n",
    "def is_cantonese(text):\n",
    "    # åˆ¤æ–­è¯„è®ºä¸­è‹±æ–‡å­—æ¯æ¯”ä¾‹\n",
    "    num_english = sum(1 for char in text if char.isalpha() and ord(char) < 128)\n",
    "    return num_english / len(text) < 0.5\n",
    "\n",
    "# å®šä¹‰é¢„å¤„ç†å‡½æ•°\n",
    "def preprocess_data(input_file, output_file):\n",
    "    # è¯»å– JSON æ•°æ®\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # æ¸…ç†å’Œè¿‡æ»¤æ•°æ®\n",
    "    cleaned_data = [\n",
    "        {\"text\": clean_text_no_emojis(item[\"text\"]), \"label\": item[\"label\"]}\n",
    "        for item in data\n",
    "        if is_cantonese(item[\"text\"]) and len(item[\"text\"]) > 5 and len(item[\"text\"]) <= 250\n",
    "    ]\n",
    "\n",
    "    # ä¿å­˜æ¸…ç†åçš„æ•°æ®\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(cleaned_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"é¢„å¤„ç†å®Œæˆï¼Œæ•°æ®é›†å·²ä¿å­˜ä¸ºï¼š{output_file}\")\n",
    "\n",
    "# å¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†åˆ†åˆ«è¿›è¡Œé¢„å¤„ç†\n",
    "preprocess_data(input_train_file, output_train_file)\n",
    "# preprocess_data(input_test_file, output_test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59536dfb",
   "metadata": {},
   "source": [
    "#### 2. with emojis ä¿ç•™è¡¨æƒ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33285e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in c:\\users\\charlotte.geng\\anaconda3\\envs\\intel_nlp\\lib\\site-packages (2.11.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dfe3d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "é¢„å¤„ç†å®Œæˆï¼Œæ•°æ®é›†å·²ä¿å­˜ä¸ºï¼šOpenrice_train_cleaned_with_emojis.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import emoji\n",
    "import re\n",
    "\n",
    "input_train_file = \"Openrice_train_balanced2.json\"\n",
    "# input_test_file = \"Openrice_test_labeled.json\" \n",
    "\n",
    "output_train_file = \"Openrice_train_cleaned_with_emojis.json\"  # Cleaned training set file\n",
    "# output_test_file = \"Openrice_test_cleaned_with_emojis.json\"  \n",
    "\n",
    "def clean_text_with_emojis(text):\n",
    "    # Remove <sssss> or other invalid placeholders\n",
    "    text = re.sub(r'<sssss>', '', text)\n",
    "    # Keep emoji, text, punctuation, and whitespace\n",
    "    text = ''.join(char for char in text if emoji.is_emoji(char) or char.isalnum() or char.isspace() or char in \".,!?\")\n",
    "    # Remove extra white space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Define language detection function (filter non-Cantonese reviews)\n",
    "def is_cantonese(text):\n",
    "    # Determine the ratio of letters in Chinese to English in a comment\n",
    "    num_english = sum(1 for char in text if char.isalpha() and ord(char) < 128)\n",
    "    return num_english / len(text) < 0.5\n",
    "\n",
    "def preprocess_data(input_file, output_file):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Clean and filter data\n",
    "    cleaned_data = [\n",
    "        {\"text\": clean_text_with_emojis(item[\"text\"]), \"label\": item[\"label\"]}\n",
    "        for item in data\n",
    "        if is_cantonese(item[\"text\"]) and len(item[\"text\"]) > 5 and len(item[\"text\"]) <= 250\n",
    "    ]\n",
    "\n",
    "    # save the cleaned data\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(cleaned_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"é¢„å¤„ç†å®Œæˆï¼Œæ•°æ®é›†å·²ä¿å­˜ä¸ºï¼š{output_file}\")\n",
    "\n",
    "# Preprocess the training and test data separately\n",
    "preprocess_data(input_train_file, output_train_file)\n",
    "# preprocess_data(input_test_file, output_test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a357bc5",
   "metadata": {},
   "source": [
    "#### The text containing emojis is further divided into two processing methods: \n",
    "#### 1. Retain the emojis themselves.  ï¼ˆNo changes to the original codeï¼‰\n",
    "#### 2. Translate emojis into text descriptions (e.g. \"ğŸ˜‰\" into \"çœ¨çœ¼å’ç¬‘\")  ï¼ˆAdditional code is required to handle emojis in the textï¼‰\n",
    "#### å¯¹äºåŒ…å«è¡¨æƒ…ç¬¦å·çš„æ–‡æœ¬ï¼Œè¿›ä¸€æ­¥åˆ†ä¸ºä¸¤ç§å¤„ç†æ–¹å¼ï¼š\n",
    "#### 1. ä¿ç•™è¡¨æƒ…ç¬¦å·æœ¬èº«ã€‚ï¼ˆåŸå§‹ä»£ç æ²¡æœ‰æ”¹å˜ï¼‰\n",
    "#### 2. å°†è¡¨æƒ…ç¬¦å·ç¿»è¯‘æˆæ–‡å­—æè¿°(ä¾‹å¦‚ï¼šå°†â€œğŸ˜‰â€è½¬æ¢ä¸º\"çœ¨çœ¼å’ç¬‘\")ï¼ˆéœ€è¦é¢å¤–ä»£ç æ¥å¤„ç†æ–‡æœ¬ä¸­çš„è¡¨æƒ…ç¬¦å·ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4ad86db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emoji è½¬ç²¤è¯­æ–‡æœ¬çš„è½¬æ¢å®Œæˆï¼Œæ•°æ®é›†å·²ä¿å­˜ä¸ºï¼šOpenrice_train_cleaned_with_emojis_texts.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "input_train_file = \"Openrice_train_cleaned_with_emojis.json\"  \n",
    "# input_test_file = \"Openrice_test_cleaned_with_emojis.json\"  \n",
    "output_train_file = \"Openrice_train_cleaned_with_emojis_texts.json\"  \n",
    "# output_test_file = \"Openrice_test_cleaned_with_emojis_texts.json\"  \n",
    "\n",
    "# Define a dictionary of emoji to Cantonese descriptions\n",
    "emoji_to_cantonese = {\n",
    "  \"ğŸ™‚\": \"å¾®å¾®ç¬‘\",\n",
    "  \"ğŸ˜Š\": \"çœ¼å¾®ç¬‘ï¼Œé¢ç´…ç´…\",\n",
    "  \"ğŸ¤—\": \"ç¬‘ä½é–‹æ‰‹æŠ±æŠ±\",\n",
    "  \"ğŸ˜‡\": \"æˆ´ä½å…‰ç’°ç¬‘\",\n",
    "  \"ğŸ˜‰\": \"çœ¨çœ¼å’ç¬‘\",\n",
    "  \"ğŸ™ƒ\": \"åè½‰å’—å˜…æ¨£\",\n",
    "  \"ğŸ˜€\": \"éœ²é½’å¤§ç¬‘\",\n",
    "  \"ğŸ˜ƒ\": \"å¤§çœ¼å¤§ç¬‘éœ²é½’\",\n",
    "  \"ğŸ˜„\": \"çœ¼ç¬‘é½’éœ²\",\n",
    "  \"ğŸ˜\": \"çœ‰é£›è‰²èˆ\",\n",
    "  \"ğŸ˜†\": \"å’ªåŸ‹çœ¼é½’éœ²\",\n",
    "  \"ğŸ˜…\": \"å’ªåŸ‹çœ¼é½’éœ²å‡ºæ±—\",\n",
    "  \"ğŸ¤£\": \"æ»¾åœ°å¤§ç¬‘åˆ°æŠ½ç­‹\",\n",
    "  \"ğŸ˜‚\": \"é–‹å¿ƒåˆ°å–Š\",\n",
    "  \"ğŸ¥²\": \"é‚Šç¬‘é‚Šå–Š\",\n",
    "  \"ğŸ¤¤\": \"æµæ™’å£æ°´\",\n",
    "  \"ğŸ˜Œ\": \"è¼•é¬†æ™’\",\n",
    "  \"ğŸ¤“\": \"æ›¸èŸ²æ¨£\",\n",
    "  \"ğŸ˜\": \"æˆ´æ™’å¢¨é¡ç¬‘\",\n",
    "  \"ğŸ¤ \": \"æˆ´ä½ç‰›ä»”å¸½é½’éœ²ç¬‘\",\n",
    "  \"ğŸ¥³\": \"æˆ´æ´¾å°å¸½æ’’èŠ±\",\n",
    "  \"â˜ºï¸\": \"å¾®å¾®ç¬‘\",\n",
    "  \"ğŸ¥°\": \"ç”œåˆ°æ¼\",\n",
    "  \"ğŸ˜\": \"çœ¼ä»”ç™¼å¿ƒå¿ƒ\",\n",
    "  \"ğŸ¤©\": \"çœ¼ä»”ç™¼æ˜Ÿæ˜Ÿ\",\n",
    "  \"ğŸ˜—\": \"å˜Ÿå˜´éŒ«äºº\",\n",
    "  \"ğŸ˜™\": \"å’ªåŸ‹çœ¼å˜Ÿå˜´éŒ«äºº\",\n",
    "  \"ğŸ˜š\": \"é–‰åŸ‹çœ¼å˜Ÿå˜´éŒ«äºº\",\n",
    "  \"ğŸ˜˜\": \"é£›å€‹é¦™å»\",\n",
    "  \"ğŸ˜³\": \"é¢ç´…æ™’\",\n",
    "  \"ğŸ¥º\": \"æ¥šæ¥šå¯æ†\",\n",
    "  \"ğŸ˜•\": \"å¥½ç–‘æƒ‘\",\n",
    "  \"ğŸ™\": \"æœ‰å•²å””é–‹å¿ƒ\",\n",
    "  \"â˜¹ï¸\": \"å””é–‹å¿ƒ\",\n",
    "  \"ğŸ˜Ÿ\": \"æ†‚å¿ƒå¿¡å¿¡\",\n",
    "  \"ğŸ˜®\": \"å¼µé–‹å˜´å·´å¥½é©šè¨\",\n",
    "  \"ğŸ˜¯\": \"çœ‰é£›è‰²èˆå¥½é©šè¨\",\n",
    "  \"ğŸ˜¦\": \"é©šåˆ°æ²®å–ªæ™’\",\n",
    "  \"ğŸ˜§\": \"çšºåŸ‹çœ‰é©šæ²®æ™’\",\n",
    "  \"ğŸ˜²\": \"å˜©å’å¤§åæ‡‰\",\n",
    "  \"ğŸ˜¨\": \"é©šåˆ°éœ‡\",\n",
    "  \"ğŸ˜°\": \"å†’å†·æ±—\",\n",
    "  \"ğŸ˜¥\": \"å¤±æœ›åˆé¬†ä¸€å£æ°£\",\n",
    "  \"ğŸ˜¢\": \"å–Šç·Š\",\n",
    "  \"ğŸ˜±\": \"å°–å«æ™’\",\n",
    "  \"ğŸ˜–\": \"å¥½å›°æƒ‘\",\n",
    "  \"ğŸ˜£\": \"æ„çœ‰è‹¦è‡‰\",\n",
    "  \"ğŸ˜\": \"å¤±æœ›é€é ‚\",\n",
    "  \"ğŸ˜“\": \"æµå†·æ±—\",\n",
    "  \"ğŸ˜©\": \"ç…©åˆ°çˆ†\",\n",
    "  \"ğŸ˜«\": \"ç…©åˆ°æ”°æ™’\",\n",
    "  \"ğŸ˜¶\": \"ç„¡è©±å¯è¬›\",\n",
    "  \"ğŸ˜\": \"ä¸€èˆ¬èˆ¬\",\n",
    "  \"ğŸ˜‘\": \"é¢ç™±æ¨£\",\n",
    "  \"ğŸ˜’\": \"å””å±‘æ¨£\",\n",
    "  \"ğŸ˜\": \"å¥¸ç¬‘\",\n",
    "  \"ğŸ¤¨\": \"å–®é‚ŠæŒ‘çœ‰\",\n",
    "  \"ğŸ™„\": \"ç¿»ç™½çœ¼\",\n",
    "  \"ğŸ˜¬\": \"å°·å°¬åˆ°çˆ†\",\n",
    "  \"ğŸ¤\": \"æ‹‰éˆå°æ™’å˜´\",\n",
    "  \"ğŸ¤¥\": \"è¬›å¤§è©±\",\n",
    "  \"ğŸ¤«\": \"å™“ï¼Œéœå•²\",\n",
    "  \"ğŸ¤­\": \"æ©ä½å˜´ç¬‘\",\n",
    "  \"ğŸ¤”\": \"è«—ç·Šè¨ˆ\",\n",
    "  \"ğŸ§\": \"æˆ´å–®ç‰‡çœ¼é¡å¥½åš´è‚…\",\n",
    "  \"ğŸ˜œ\": \"çœ¨çœ¼ä¼¸èˆŒé ­\",\n",
    "  \"ğŸ˜\": \"å’ªåŸ‹çœ¼ä¼¸èˆŒé ­\",\n",
    "  \"ğŸ˜›\": \"ä¼¸èˆŒé ­\",\n",
    "  \"ğŸ˜‹\": \"èˆ”èˆ”èˆŒé ­\",\n",
    "  \"ğŸ¤ª\": \"ç™²ç™²å“‹\",\n",
    "  \"ğŸ¤‘\": \"çœ¼ä»”ç™¼éŒ¢éŒ¢\",\n",
    "  \"ğŸ˜ \": \"å¬²å¬²è±¬\",\n",
    "  \"ğŸ˜¡\": \"ç´…æ™’é¢å¬²çˆ†\",\n",
    "  \"ğŸ¤¬\": \"å¬²åˆ°çˆ†ç²—\",\n",
    "  \"ğŸ˜¤\": \"å™´æ°£å¬²\",\n",
    "  \"ğŸ˜ˆ\": \"å£ç¬‘æˆ´è§’\",\n",
    "  \"ğŸ‘¿\": \"å¬²åˆ°ç¾å½¢\",\n",
    "  \"ğŸ˜´\": \"ç“è‘—å’—\",\n",
    "  \"ğŸ˜ª\": \"æ‰“çŒç¡\",\n",
    "  \"ğŸ˜µ\": \"é ­æšˆ\",\n",
    "  \"ğŸ˜µâ€ğŸ’«\": \"é ­æšˆç›®çœ©\",\n",
    "  \"ğŸ¤¯\": \"è…¦è¢‹çˆ†æ™’ç‚¸\",\n",
    "  \"ğŸ¤’\": \"ç™¼ç·Šç‡’\",\n",
    "  \"ğŸ¤•\": \"é ­éƒ¨æ’å‚·\",\n",
    "  \"ğŸ¤¢\": \"ä½œå˜”\",\n",
    "  \"ğŸ¤®\": \"å˜”æ™’\",\n",
    "  \"ğŸ¤§\": \"æ‰“æ™’ä¹å—¤\",\n",
    "  \"ğŸ¥µ\": \"ç†±åˆ°æšˆ\",\n",
    "  \"ğŸ¥¶\": \"å‡åˆ°éœ‡\",\n",
    "  \"ğŸ˜·\": \"æˆ´ä½å£ç½©\", \n",
    "  \"ğŸ˜­\": \"å–Š\",\n",
    "  \"ğŸ˜”\": \"å¿ƒå””èˆ’æœ\",    \n",
    "  \"â¤ï¸\": \"é¾æ„\",\n",
    "  \"ğŸ’”\": \"å¿ƒç—›\",    \n",
    "  \"ğŸ’•\": \"å¥½ç”œèœœ\",    \n",
    "  \"ğŸ”¥\": \"ç«çˆ†\",\n",
    "  \"ğŸ‘\": \"è®šå¥½\", \n",
    "  \"ğŸ‘Œ\": \"å¥½æ­£\",\n",
    "  \"ğŸ´\": \"é£Ÿå¥½å‘³\",\n",
    "  \"â­\": \"æ˜Ÿæ˜Ÿ\",\n",
    "  \"ğŸ®\": \"ç‰›è‚‰\",\n",
    "  \"ğŸ·\": \"è±¬è‚‰\",\n",
    "  \"ğŸ”\": \"é›è‚‰\",\n",
    "  \"ğŸ‘\": \"ç¾Šè‚‰\",\n",
    "  \"ğŸŸ\": \"é­š\",\n",
    "  \"ğŸ¦\": \"è¦\",\n",
    "  \"ğŸ¦€\": \"èŸ¹\",\n",
    "  \"ğŸ¦Œ\": \"é¹¿è‚‰\",\n",
    "  \"ğŸ¦†\": \"é´¨è‚‰\"\n",
    "}\n",
    "\n",
    "# Conversion function: Convert emojis to Cantonese descriptions\n",
    "def convert_emojis_to_cantonese(text):\n",
    "    # Replace emojis in the text with their Cantonese counterparts using a dictionary\n",
    "    for emoji_char, cantonese_word in emoji_to_cantonese.items():\n",
    "        text = text.replace(emoji_char, cantonese_word)\n",
    "    return text\n",
    "\n",
    "def preprocess_emojis_to_cantonese(input_file, output_file):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Convert emojis in each review\n",
    "    converted_data = [\n",
    "        {\"text\": convert_emojis_to_cantonese(item[\"text\"]), \"label\": item[\"label\"]}\n",
    "        for item in data\n",
    "    ]\n",
    "\n",
    "    # save the transformed data\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(converted_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"emoji è½¬ç²¤è¯­æ–‡æœ¬çš„è½¬æ¢å®Œæˆï¼Œæ•°æ®é›†å·²ä¿å­˜ä¸ºï¼š{output_file}\")\n",
    "\n",
    "# Convert training and test data\n",
    "preprocess_emojis_to_cantonese(input_train_file, output_train_file)\n",
    "# preprocess_emojis_to_cantonese(input_test_file, output_test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fb4dc8",
   "metadata": {},
   "source": [
    "### 6. Convert the data set to Alpaca format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a9e019e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è½¬æ¢å®Œæˆï¼ŒAlpaca æ ¼å¼æ•°æ®å·²ä¿å­˜ä¸ºï¼šOpenrice_train_alpaca_with_emojis.json\n",
      "è½¬æ¢å®Œæˆï¼ŒAlpaca æ ¼å¼æ•°æ®å·²ä¿å­˜ä¸ºï¼šOpenrice_train_alpaca_no_emojis.json\n",
      "è½¬æ¢å®Œæˆï¼ŒAlpaca æ ¼å¼æ•°æ®å·²ä¿å­˜ä¸ºï¼šOpenrice_train_alpaca_with_emojis_texts.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "input_train_no_emojis = \"Openrice_train_cleaned_no_emojis.json\"\n",
    "# input_test_no_emojis = \"Openrice_test_cleaned_no_emojis.json\"\n",
    "input_train_with_emojis = \"Openrice_train_cleaned_with_emojis.json\"\n",
    "# input_test_with_emojis = \"Openrice_test_cleaned_with_emojis.json\"\n",
    "input_train_with_emojis_texts = \"Openrice_train_cleaned_with_emojis_texts.json\" \n",
    "# input_test_with_emojis_texts = \"Openrice_test_cleaned_with_emojis_texts.json\"  \n",
    "\n",
    "# Output file path (after converting to Alpaca)\n",
    "output_train_no_emojis = \"Openrice_train_alpaca_no_emojis.json\"\n",
    "# output_test_no_emojis = \"Openrice_test_alpaca_no_emojis.json\"\n",
    "output_train_with_emojis = \"Openrice_train_alpaca_with_emojis.json\"\n",
    "# output_test_with_emojis = \"Openrice_test_alpaca_with_emojis.json\"\n",
    "output_train_with_emojis_texts = \"Openrice_train_alpaca_with_emojis_texts.json\" \n",
    "# output_test_with_emojis_texts = \"Openrice_test_alpaca_with_emojis_texts.json\"\n",
    "\n",
    "def convert_to_alpaca_format(input_file, output_file):\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    alpaca_data = [\n",
    "        {\n",
    "            \"instruction\": \"ä½ æ˜¯ä¸€ä¸ªç»è¿‡è®­ç»ƒçš„äººå·¥æ™ºèƒ½ï¼Œå¯ä»¥åˆ†ææ–‡æœ¬è¾“å…¥ï¼Œå¹¶æ ¹æ®ä¸Šä¸‹æ–‡å°†å…¶åˆ†ç±»ä¸ºæœ€åˆé€‚çš„æƒ…æ„Ÿç±»å‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä»”ç»†è¯„ä¼°è¾“å…¥å¹¶ç¡®å®šè¾“å…¥å±äºå“ªç§æƒ…ç»ªã€‚ä»ä»¥ä¸‹æƒ…ç»ªä¸­é€‰æ‹©ï¼š1. â€œæ­£é¢â€ï¼Œ2.â€œè´Ÿé¢â€ã€‚åªæä¾›æƒ…ç»ªçš„åç§°ä½œä¸ºè¾“å‡ºï¼Œä¸æä¾›é¢å¤–çš„è§£é‡Šã€‚\",\n",
    "            \"input\": item[\"text\"],  # Comment text as input\n",
    "            \"output\": str(item[\"label\"])  # Emotion labels as output\n",
    "        }\n",
    "        for item in data\n",
    "    ]\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(alpaca_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"è½¬æ¢å®Œæˆï¼ŒAlpaca æ ¼å¼æ•°æ®å·²ä¿å­˜ä¸ºï¼š{output_file}\")\n",
    "\n",
    "convert_to_alpaca_format(input_train_with_emojis, output_train_with_emojis)\n",
    "# convert_to_alpaca_format(input_test_with_emojis, output_test_with_emojis)\n",
    "convert_to_alpaca_format(input_train_no_emojis, output_train_no_emojis)\n",
    "# convert_to_alpaca_format(input_test_no_emojis, output_test_no_emojis)\n",
    "convert_to_alpaca_format(input_train_with_emojis_texts, output_train_with_emojis_texts)\n",
    "# convert_to_alpaca_format(input_test_with_emojis_texts, output_test_with_emojis_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0799d42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35cb0cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a978e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (intel_nlp)",
   "language": "python",
   "name": "intel_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
